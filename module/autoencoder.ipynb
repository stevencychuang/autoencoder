{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAll rights can reffer to the LICENSE.md.\\n\\nCreated on July 19, 2018.\\n\\nThis module provides the several classes of autoencoder series.\\nThe use could simply use these API without defining the structure by oneself.\\n\\n@author: steven.cy.chuang\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "All rights can reffer to the LICENSE.md.\n",
    "\n",
    "Created on July 19, 2018.\n",
    "\n",
    "This module provides the several classes of autoencoder series.\n",
    "The use could simply use these API without defining the structure by oneself.\n",
    "\n",
    "@author: steven.cy.chuang\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from time import time\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Conv2D, Conv2DTranspose, Activation, Flatten, Reshape, LeakyReLU, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Model, model_from_json\n",
    "from keras import backend as K\n",
    "from keras.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE():\n",
    "    _stdEps = 1.0\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dimInput, \n",
    "                 layerDense=[64, 2], actDense='leaky_relu', dropoutDense=0.5,\n",
    "                 ratRecon=0.998\n",
    "                ):\n",
    "        \"\"\"\n",
    "        The basic properties and pipeline will be defined in the initialization.\n",
    "        It should be noted that layerDense defines the first half(encoder) of network. \n",
    "        The decoder will be reflected structure.\n",
    "        For example, [64, 16, 2] means the nodes of decoder will be [2, 16, 64]. \n",
    "        There is another parameter should noted that ratRecon=0.5 doesn't mean the effect is half.\n",
    "        Because KL loss and reconstruction loss are not the same scale.\n",
    "        Args:\n",
    "            dimInput (int): the number of input dimension. All features are flatten as a vector.\n",
    "            layerDense (list[int]): the numbers of each dense layer. Default is [64, 2].\n",
    "            actDense (string): the activation function. Default is 'leaky_relu'.\n",
    "            dropoutDense (float): the dropout layer. Default is 0.5.\n",
    "            ratRecon (float): the parameter for tuning the effects between KL loss and reconstruction loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize some setting \n",
    "        self._dimInput = dimInput # all features are flatten as a vector\n",
    "        self._inputs = Input(shape=(dimInput,)) \n",
    "        self._dimEncode = layerDense[-1]\n",
    "        self._ratRecon = ratRecon\n",
    "        \n",
    "        self._encoding(layerDense, actDense, dropoutDense)\n",
    "        \n",
    "        self._decoding(layerDense, actDense, dropoutDense)\n",
    "        \n",
    "        self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)), name='autoencoder')\n",
    "\n",
    "        \n",
    "    def _encoding(self, layerDense, actDense, dropoutDense):\n",
    "        dimEncode = self._dimEncode\n",
    "        x = self._inputs\n",
    "\n",
    "        # Stack of Dense layers\n",
    "        for numFilt in layerDense[:-1]:\n",
    "            if actDense == 'leaky_relu':\n",
    "                x = Dense(numFilt)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Dense(numFilt, activation=actDense)(x)\n",
    "            if dropoutDense > 0:\n",
    "                x = Dropout(0.5)(x)\n",
    "        self._zMean = Dense(self._dimEncode)(x)\n",
    "        self._zSigmaLog = Dense(self._dimEncode)(x) # log for linear dense\n",
    "\n",
    "        # Define the sampling function for the sampling layer.\n",
    "        # Note that the function must be in the same location with encoding for saving/loadind model.\n",
    "        def sampling(args, stdEps):\n",
    "            zMean, zSigmaLog = args\n",
    "            epsilon = K.random_normal(shape=(K.shape(zMean)[0], K.shape(zMean)[1]),\n",
    "                                      mean=0., stddev=stdEps)\n",
    "            return zMean + K.exp(zSigmaLog) * epsilon  \n",
    "        \n",
    "        # Construct the latent as the output and build the encorder pipeline\n",
    "        z = Lambda(sampling, arguments={'stdEps':self._stdEps})([self._zMean, self._zSigmaLog])\n",
    "        self.encoder = Model(self._inputs, z, name='encoder')\n",
    "        \n",
    "        \n",
    "    def _decoding(self, layerDense, actDense, dropoutDense):\n",
    "         # Build the Decoder Model\n",
    "        inputLatent = Input(shape=(self._dimEncode,), name='decoder_input')\n",
    "        x = inputLatent\n",
    "        for numFilt in layerDense[-2::-1]:\n",
    "            if actDense == 'leaky_relu':\n",
    "                x = Dense(numFilt)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Dense(numFilt, activation=actDense)(x)\n",
    "            if dropoutDense > 0:\n",
    "                x = Dropout(0.5)(x)\n",
    "            \n",
    "        # Reconstruct the pixels as the output and build the decorder pipeline\n",
    "        outputs = Dense(self._dimInput, activation='sigmoid', name='decoder_output')(x)\n",
    "        self.decoder = Model(inputLatent, outputs, name='decoder')\n",
    "        \n",
    "        \n",
    "    def _lossVAE(self, tensorInput, tensorDecode):\n",
    "        zMean = self._zMean\n",
    "        zSigmaLog = self._zSigmaLog\n",
    "        ratRecon = self._ratRecon\n",
    "        \n",
    "        lossRecon =  binary_crossentropy(K.flatten(tensorInput), K.flatten(tensorDecode))\n",
    "#         lossRecon =  mean_squared_error(K.flatten(tensorInput), K.flatten(tensorDecode))\n",
    "        lossKL = - 0.5 * K.sum(1 + 2 * zSigmaLog - K.square(zMean) - K.square(K.exp(zSigmaLog)), axis=-1)\n",
    "        return ratRecon * lossRecon + (1 - ratRecon) * lossKL\n",
    "        \n",
    "        \n",
    "    def fit(self,\n",
    "            xTrain, xValid,\n",
    "            numEpochs=50, sizeBatch=32, nameOptim='adam', metrics=None,\n",
    "            pathTempBest=None, patience=3,\n",
    "           ):\n",
    "        \"\"\"\n",
    "        The method is for training process. \n",
    "        The users can call this method easily just putting training and validation datasets.\n",
    "        The dimension of dataset is determined by [#instance, *dimInput].\n",
    "        For example, dimInput is flatten as a number and the dimension of dataset is [#instance, #feature]\n",
    "        If dimInput is a list to represent [width, height, channels], the dimension of dataset is [#instance, width, height, channels]\n",
    "        Args:\n",
    "            xTrain (numpy ndarray): the training dataset.\n",
    "            xValid (numpy ndarray): the validation dataset.\n",
    "            numEpochs (int): the maximal epochs for training. Default is 50.\n",
    "            sizeBatch (int): the batch size. Default is 32.\n",
    "            nameOptim (string): the method for optimization. Default is adam.\n",
    "            metrics (list(string or keras metrics)): the usage is the same with keras metrics for compile \n",
    "            pathTempBest (string): the temperory path of the best model for early-stop. Default None means without early-stop. \n",
    "            patience (int): the times of epochs to allow further trying if current loss is not better than the best. \n",
    "        Returns:\n",
    "            history (keras.callbacks.History): the learning curving for the training process\n",
    "            timeTrain (float): the consuming time of the training \n",
    "        \"\"\"\n",
    "        self.autoencoder.compile(optimizer=nameOptim, loss=self._lossVAE, metrics=metrics)\n",
    "\n",
    "        if pathTempBest is None:\n",
    "            callbacks = None\n",
    "        else:\n",
    "            name_temp = \"AutoEncoder\" + str(time()) # use timestamp as unique name\n",
    "            cbEarlyStop = EarlyStopping(monitor='val_loss', patience=patience, verbose=1, mode='auto')\n",
    "            chkpt = pathTempBest + \"/\" + name_temp + \".hdf5\"\n",
    "            cbCheckPoint = ModelCheckpoint(filepath = chkpt, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "            callbacks = [cbEarlyStop, cbCheckPoint]\n",
    "        \n",
    "        # Train the autoencoder\n",
    "        tic = time()\n",
    "        history = self.autoencoder.fit(xTrain, xTrain,\n",
    "                                       epochs=numEpochs,\n",
    "                                       batch_size=sizeBatch, shuffle=True,\n",
    "                                       callbacks=callbacks,\n",
    "                                       validation_data=(xValid, xValid)\n",
    "                                      )\n",
    "        timeTrain = time() - tic\n",
    "        \n",
    "        # Assure the models are resuming from the best models\n",
    "        if pathTempBest is not None:\n",
    "            self.autoencoder = keras.models.load_model(chkpt, custom_objects={\"_lossVAE\": self._lossVAE})\n",
    "            self.encoder = self.autoencoder.layers[1]\n",
    "            self.decoder = self.autoencoder.layers[2]\n",
    "        \n",
    "        return history, timeTrain\n",
    "    \n",
    "    def save(self, pathFolder):\n",
    "        \"\"\"\n",
    "        Deprecated! Because the availability for saving/loading model is limited.\n",
    "        The method is to save the models of autoencoders as several hdf5 files in a given path of the folder.\n",
    "        Args:\n",
    "            pathFile (string): the given path of the folder where contains encoder, decoder, and autoencoder\n",
    "        Returns:\n",
    "            msg (string): the message for the saving process\n",
    "        \"\"\"\n",
    "        # Create the message for saving model\n",
    "        msg = ''\n",
    "        if os.path.exists(pathFolder):\n",
    "            msg += 'There is a existing folder.\\r\\n'\n",
    "        else:\n",
    "            os.makedirs(pathFolder)\n",
    "            msg += 'Create a new folder.\\r\\n'\n",
    "        \n",
    "        # Save the models of encoder and decoder with save()\n",
    "        self.encoder.save(pathFolder+'/encoder.h5')\n",
    "        self.decoder.save(pathFolder+'/decoder.h5')\n",
    "        \n",
    "        # Save the model of autoencoder with json and save_weights(). \n",
    "        # Because autoencoder contains special loss function and sample function.\n",
    "        self.autoencoder.save_weights(pathFolder+'/weightAutoencoder.h5')\n",
    "        with open(pathFolder+'/configAutoencoder.json', 'w') as jsonFile:\n",
    "            jsonFile.write(self.autoencoder.to_json())\n",
    "        msg += 'successful.\\r\\n'\n",
    "        return msg\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(pathFolder):\n",
    "        \"\"\"\n",
    "        The method is to save the models of autoencoders as several hdf5 files in a given path of the folder.\n",
    "        Args:\n",
    "            pathFile (string): the given path of the folder where contains encoder, decoder, and autoencoder\n",
    "        Returns:\n",
    "            encoder (keras model): the model of encoder\n",
    "            decoder (keras model): the model of decoder\n",
    "            autoencoder (keras model) : the model of autoencoder. autoencoder.predict(x) is equivalent to decoder.predict(encoder.predict(x))\n",
    "        \"\"\"\n",
    "        encoder = keras.models.load_model(pathFolder+'/encoder.h5')\n",
    "        decoder = keras.models.load_model(pathFolder+'/decoder.h5')\n",
    "        \n",
    "        with open(pathFolder+'/configAutoencoder.json', 'r') as jsonFile:                              \n",
    "            jsonConfig = jsonFile.readlines()[0]\n",
    "        autoencoder = model_from_json(jsonConfig)\n",
    "        autoencoder.load_weights(pathFolder+'/weightAutoencoder.h5')\n",
    "        return encoder, decoder, autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutinal VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvVAE(VAE):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 dimInput, \n",
    "                 layerConv=[8, 32], sizeKernel=3, strides=2, actConv='leaky_relu', padding='same',\n",
    "                 layerDense=[64, 2], actDense='leaky_relu', dropoutDense=0.5,\n",
    "                 ratRecon=0.998):\n",
    "        \"\"\"\n",
    "        The basic properties and pipeline will be defined in the initialization.\n",
    "        The dimension of input should be a form of a picture presented by a list [width, height, channels].\n",
    "        It should be noted that layerDense defines the first half(encoder) of network. \n",
    "        The decoder will be reflected structure.\n",
    "        For example, [64, 16, 2] means the nodes of decoder will be [2, 16, 64].\n",
    "        It is similar for layerConv but decoder is not purely symmetric for convolution layers for this version.\n",
    "        There is another parameter should noted that ratRecon=0.5 doesn't mean the effect is half.\n",
    "        Because KL loss and reconstruction loss are not the same scale.\n",
    "        Args:\n",
    "            dimInput (list[int]): the dimension of input. E.g. [32, 28, 3] means 32 by 28 RGB pixels.\n",
    "            layerConv (list[int]): the numbers of each convolution layer. Default is [8, 32].\n",
    "            sizeKernel (int): the size of filter kernel. Default 3 means 3 by 3.\n",
    "            strides (int): the stride for convolution. Default is 2.\n",
    "            actConv (string): the activation function of each convolution layer. Default is 'leaky_relu'.\n",
    "            padding (string): the padding method for convolution. Default is 'same'.\n",
    "            layerDense (list[int]): the numbers of each dense layer. Default is [64, 2].\n",
    "            actDense (string): the activation function of each dense layer. Default is 'leaky_relu'.\n",
    "            dropoutDense (float): the dropout layer. Default is 0.5.\n",
    "            ratRecon (float): the parameter for tuning the effects between KL loss and reconstruction loss.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize some setting \n",
    "        self._dimInput = dimInput # dimInput is (width, height, channels)\n",
    "        self._inputs = Input(shape=(dimInput)) \n",
    "        self._dimEncode = layerDense[-1]\n",
    "        self._ratRecon = ratRecon\n",
    "        \n",
    "        self._encoding(layerConv, sizeKernel, strides, actConv, padding,\n",
    "                      layerDense, actDense, dropoutDense\n",
    "                     )\n",
    "        \n",
    "        self._decoding(layerConv, sizeKernel, strides, actConv, padding,\n",
    "                      layerDense, actDense, dropoutDense\n",
    "                     )\n",
    "        \n",
    "        self.autoencoder = Model(self._inputs, self.decoder(self.encoder(self._inputs)), name='autoencoder')\n",
    "        \n",
    "\n",
    "        \n",
    "    def _encoding(self, \n",
    "                 layerConv, sizeKernel, strides, actConv, padding,\n",
    "                 layerDense, actDense, dropoutDense\n",
    "                ):\n",
    "        dimEncode = self._dimEncode\n",
    "        x = self._inputs\n",
    "        # Stack of Conv2D layers\n",
    "        for filters in layerConv:\n",
    "            if actConv == 'leaky_relu':\n",
    "                x = Conv2D(filters=filters,\n",
    "                           kernel_size=sizeKernel,\n",
    "                           strides=strides,\n",
    "                           padding=padding)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Conv2D(filters=filters,\n",
    "                           kernel_size=sizeKernel,\n",
    "                           strides=strides,\n",
    "                           activation=actConv,\n",
    "                           padding=padding)(x)\n",
    "\n",
    "        # Shape info needed to build Decoder Model\n",
    "        self._shapeLastConv = K.int_shape(x)\n",
    "\n",
    "        # Stack of Dense layers\n",
    "        x = Flatten()(x)\n",
    "        for numFilt in layerDense[:-1]:\n",
    "            if actDense == 'leaky_relu':\n",
    "                x = Dense(numFilt)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Dense(numFilt, activation=actDense)(x)\n",
    "            if dropoutDense > 0:\n",
    "                x = Dropout(0.5)(x)\n",
    "        self._zMean = Dense(dimEncode)(x)\n",
    "        self._zSigmaLog = Dense(dimEncode)(x) # log for linear dense\n",
    "\n",
    "        # Define the sampling function for the sampling layer.\n",
    "        # Note that the function must be in the same location with encoding for saving/loadind model.\n",
    "        def sampling(args, stdEps):\n",
    "            zMean, zSigmaLog = args\n",
    "            epsilon = K.random_normal(shape=(K.shape(zMean)[0], K.shape(zMean)[1]),\n",
    "                                      mean=0., stddev=stdEps)\n",
    "            return zMean + K.exp(zSigmaLog) * epsilon  \n",
    "        \n",
    "        # Construct the latent as the output and build the encorder pipeline\n",
    "        z = Lambda(sampling, arguments={'stdEps':self._stdEps})([self._zMean, self._zSigmaLog])\n",
    "        self.encoder = Model(self._inputs, z, name='encoder')\n",
    "\n",
    "        \n",
    "    def _decoding(self,\n",
    "                 layerConv, sizeKernel, strides, actConv, padding,\n",
    "                 layerDense, actDense, dropoutDense\n",
    "                ):\n",
    "        \n",
    "        shapeLastConv = self._shapeLastConv\n",
    "         # Build the Decoder Model\n",
    "        inputLatent = Input(shape=(self._dimEncode,), name='decoder_input')\n",
    "        x = inputLatent\n",
    "        for numFilt in layerDense[-2::-1]:\n",
    "            if actDense == 'leaky_relu':\n",
    "                x = Dense(numFilt)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Dense(numFilt, activation=actDense)(x)\n",
    "        if dropoutDense > 0:\n",
    "                x = Dropout(0.5)(x)\n",
    "        \n",
    "        x = Dense(shapeLastConv[1] * shapeLastConv[2] * shapeLastConv[3])(x)\n",
    "        x = Reshape((shapeLastConv[1], shapeLastConv[2], shapeLastConv[3]))(x)\n",
    "\n",
    "        # Stack of Transposed Conv2D layers\n",
    "        for numFilt in layerConv[::-1]:\n",
    "            if actConv == 'leaky_relu':\n",
    "                x = Conv2DTranspose(filters=numFilt,\n",
    "                                    kernel_size=sizeKernel,\n",
    "                                    strides=strides,\n",
    "                                    padding=padding)(x)\n",
    "                x = LeakyReLU()(x)\n",
    "            else:\n",
    "                x = Conv2DTranspose(filters=numFilt,\n",
    "                                    kernel_size=sizeKernel,\n",
    "                                    strides=strides,\n",
    "                                    activation=actConv,\n",
    "                                    padding=padding)(x)\n",
    "        # Build the Conv2DTranspose layer for the pixel dimension\n",
    "        x = Conv2DTranspose(filters=self._dimInput[-1],\n",
    "                            kernel_size=sizeKernel,\n",
    "#                             strides=strides,\n",
    "                            padding=padding)(x)\n",
    "\n",
    "        # Reconstruct the pixels as the output and build the decorder pipeline\n",
    "        outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "        self.decoder = Model(inputLatent, outputs, name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "60000/60000 [==============================] - 2s 42us/step - loss: 0.3321 - val_loss: 0.2613\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.26130, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 2/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2587 - val_loss: 0.2451\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.26130 to 0.24511, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 3/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2487 - val_loss: 0.2397\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24511 to 0.23965, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 4/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2446 - val_loss: 0.2374\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23965 to 0.23739, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 5/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2427 - val_loss: 0.2363\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23739 to 0.23628, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 6/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2418 - val_loss: 0.2357\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.23628 to 0.23575, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 7/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2406 - val_loss: 0.2350\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23575 to 0.23499, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 8/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2399 - val_loss: 0.2342\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23499 to 0.23421, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 9/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2394 - val_loss: 0.2332\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23421 to 0.23319, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 10/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2386 - val_loss: 0.2326\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23319 to 0.23263, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 11/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2381 - val_loss: 0.2314\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.23263 to 0.23135, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 12/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2361 - val_loss: 0.2232\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.23135 to 0.22317, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 13/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2314 - val_loss: 0.2181\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22317 to 0.21815, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 14/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2287 - val_loss: 0.2156\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.21815 to 0.21563, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 15/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2271 - val_loss: 0.2146\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.21563 to 0.21457, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 16/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2264 - val_loss: 0.2137\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.21457 to 0.21372, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 17/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2254 - val_loss: 0.2129\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.21372 to 0.21291, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 18/200\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2247 - val_loss: 0.2126\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21291 to 0.21264, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 19/200\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2243 - val_loss: 0.2121\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.21264 to 0.21215, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 20/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2238 - val_loss: 0.2121\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.21215 to 0.21212, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 21/200\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2232 - val_loss: 0.2115\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.21212 to 0.21150, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 22/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2229 - val_loss: 0.2111\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.21150 to 0.21113, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 23/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2225 - val_loss: 0.2108\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.21113 to 0.21075, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 24/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2223 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.21075 to 0.21072, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 25/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2218 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.21072 to 0.21066, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 26/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2219 - val_loss: 0.2107\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.21066\n",
      "Epoch 27/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2218 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.21066 to 0.21030, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 28/200\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2217 - val_loss: 0.2098\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.21030 to 0.20983, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 29/200\n",
      "60000/60000 [==============================] - 1s 22us/step - loss: 0.2214 - val_loss: 0.2104\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.20983\n",
      "Epoch 30/200\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2211 - val_loss: 0.2103\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.20983\n",
      "Epoch 31/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2209 - val_loss: 0.2097\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.20983 to 0.20975, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 32/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2208 - val_loss: 0.2092\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.20975 to 0.20921, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 33/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2207 - val_loss: 0.2093\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.20921\n",
      "Epoch 34/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2204 - val_loss: 0.2092\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.20921 to 0.20919, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 35/200\n",
      "60000/60000 [==============================] - 1s 19us/step - loss: 0.2201 - val_loss: 0.2088\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.20919 to 0.20885, saving model to ../model/temp//AutoEncoder1535624135.576238.hdf5\n",
      "Epoch 36/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2200 - val_loss: 0.2093\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.20885\n",
      "Epoch 37/200\n",
      "60000/60000 [==============================] - 1s 21us/step - loss: 0.2198 - val_loss: 0.2093\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.20885\n",
      "Epoch 38/200\n",
      "60000/60000 [==============================] - 1s 20us/step - loss: 0.2199 - val_loss: 0.2090\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.20885\n",
      "Epoch 00038: early stopping\n",
      "[array([[ 0.01942308, -0.01637648,  0.04963167, ...,  0.05502933,\n",
      "         0.02832877, -0.02377152],\n",
      "       [ 0.00511583, -0.0095196 , -0.03313178, ...,  0.03536819,\n",
      "        -0.04230474,  0.04265615],\n",
      "       [ 0.03133994,  0.03281501,  0.04295759, ...,  0.02680593,\n",
      "         0.07079177, -0.02597428],\n",
      "       ...,\n",
      "       [ 0.02833071, -0.01984979,  0.04468694, ...,  0.00930388,\n",
      "         0.00944376,  0.06359959],\n",
      "       [ 0.05768855,  0.05408427,  0.02434234, ...,  0.07189935,\n",
      "        -0.01539868,  0.01984672],\n",
      "       [-0.05383008, -0.04783966,  0.05436985, ...,  0.03914557,\n",
      "        -0.03687038,  0.07381545]], dtype=float32), array([ 0.09465072, -0.15730569,  0.21869867, -0.06320772, -0.11140399,\n",
      "        0.08717389,  0.16540082,  0.12753946, -0.00387606, -0.1864578 ,\n",
      "        0.11311536,  0.0811394 , -0.00742846,  0.1069857 ,  0.01162779,\n",
      "        0.12404458, -0.15340918,  0.14072473,  0.09767759,  0.07151628,\n",
      "        0.07203801, -0.17904828,  0.0937025 ,  0.1473709 ,  0.16967495,\n",
      "       -0.21412274,  0.09325778,  0.14977932, -0.02226155,  0.16797476,\n",
      "       -0.14431515,  0.07762944,  0.12944214,  0.168555  ,  0.17231274,\n",
      "       -0.1568747 ,  0.03547938,  0.16523239,  0.22291642,  0.16876325,\n",
      "        0.18671465,  0.05283668,  0.06264372,  0.09227824,  0.04714647,\n",
      "       -0.10694505,  0.07719709, -0.16716313,  0.23075414, -0.06274865,\n",
      "        0.14794426,  0.08812623,  0.15779725,  0.19742957,  0.12002548,\n",
      "        0.11851352,  0.03212852,  0.17074944, -0.10968558,  0.14819731,\n",
      "        0.19022335,  0.0409871 ,  0.14182939, -0.10383014,  0.10397576,\n",
      "        0.21930364,  0.15131228,  0.19936417,  0.09062594,  0.12868506,\n",
      "        0.1615984 , -0.1469531 ,  0.10891213, -0.14506784,  0.17584032,\n",
      "        0.10464446, -0.11091968,  0.1351089 , -0.08274768,  0.19078858,\n",
      "       -0.12961726, -0.23431864,  0.11840182,  0.24127291,  0.14143567,\n",
      "        0.0289651 , -0.2010589 , -0.0576005 ,  0.03188845, -0.09033181,\n",
      "        0.21273878,  0.16203201,  0.19190197, -0.21943271, -0.15643421,\n",
      "       -0.16923662, -0.07846136, -0.117305  , -0.1085971 , -0.06505262,\n",
      "        0.03206822, -0.10888574,  0.09472439,  0.19186497,  0.16367027,\n",
      "        0.16128257,  0.13664125, -0.1335215 ,  0.03784543, -0.16551358,\n",
      "        0.06384423,  0.16564363,  0.20518357,  0.20723319,  0.07666391,\n",
      "        0.12795244, -0.05519047,  0.12219828,  0.20827219,  0.23533183,\n",
      "       -0.13890165, -0.04852342,  0.20714116,  0.03070172,  0.12014479,\n",
      "        0.04911704, -0.118513  ,  0.02272264,  0.18835631, -0.14438136,\n",
      "        0.1428351 ,  0.10279403, -0.00491383,  0.15072232, -0.10282673,\n",
      "       -0.03817836,  0.19642201,  0.14114264,  0.16338396,  0.08412792,\n",
      "        0.18098116, -0.00262571,  0.16844015,  0.14783752,  0.14307973,\n",
      "       -0.00497948,  0.18970437,  0.19771378,  0.25433284,  0.11424178,\n",
      "        0.05230851,  0.2058123 ,  0.07755972,  0.00850485,  0.06129335,\n",
      "        0.08602228,  0.06326696,  0.1651017 ,  0.19051598,  0.11654454,\n",
      "       -0.18326633,  0.23499762,  0.18882313,  0.06992662,  0.22952923,\n",
      "        0.20774487, -0.08109801,  0.1729012 ,  0.16268887,  0.18985607,\n",
      "        0.01314461,  0.13809833,  0.08163654,  0.1466879 ,  0.18196715,\n",
      "        0.08074977,  0.067252  ,  0.23013307,  0.10536445,  0.13153441,\n",
      "        0.2591509 ,  0.21917686,  0.14264819,  0.12471803,  0.11244339,\n",
      "        0.16175635,  0.1822915 ,  0.17441532,  0.21370514, -0.18450163,\n",
      "       -0.01846193,  0.18749665,  0.11556555, -0.09746028,  0.1887581 ,\n",
      "        0.18582737,  0.23032187,  0.21070527,  0.11608684,  0.14251162,\n",
      "        0.14017674,  0.05471796, -0.01652494,  0.15552919,  0.22602154,\n",
      "       -0.10732976,  0.20889369,  0.18962176,  0.03886463, -0.19275811,\n",
      "        0.11668557,  0.18063387,  0.10619407,  0.1248677 ,  0.02009955,\n",
      "       -0.11779146, -0.13736199,  0.17189918,  0.14807518,  0.16425209,\n",
      "        0.10789347,  0.14136736,  0.12730166,  0.08857992,  0.13413814,\n",
      "        0.09484008, -0.02660756,  0.06917392,  0.19244607, -0.16664936,\n",
      "       -0.15300174,  0.12476829,  0.1315192 ,  0.0751619 ,  0.07296885,\n",
      "        0.21408123,  0.12765095,  0.12902239,  0.2260284 ,  0.131898  ,\n",
      "        0.09438161, -0.02589222,  0.22728959,  0.06090361,  0.20028865,\n",
      "       -0.0736005 ,  0.10224445,  0.18094511,  0.11601683, -0.03998694,\n",
      "       -0.10981879,  0.16102545,  0.08594335,  0.0719253 ,  0.08704018,\n",
      "       -0.13930248], dtype=float32), array([[ 0.06699331,  0.01184602,  0.05100449, ..., -0.06163816,\n",
      "        -0.07211773, -0.00747125],\n",
      "       [ 0.00909219, -0.02272595, -0.09029504, ...,  0.0267797 ,\n",
      "         0.03056426,  0.00917366],\n",
      "       [-0.0227399 ,  0.05687579,  0.01163492, ...,  0.08550773,\n",
      "        -0.04673566,  0.08356057],\n",
      "       ...,\n",
      "       [ 0.03947027,  0.06216802,  0.10369278, ..., -0.10362253,\n",
      "        -0.02780734,  0.05851398],\n",
      "       [ 0.14158212, -0.02063389, -0.01735178, ...,  0.05660152,\n",
      "        -0.00520821, -0.01775711],\n",
      "       [-0.02693528, -0.0245496 , -0.07271186, ...,  0.00479501,\n",
      "         0.11940953, -0.01329715]], dtype=float32), array([ 0.10911783, -0.01370669,  0.028279  ,  0.01127233, -0.07824097,\n",
      "        0.13711749, -0.04027968,  0.06599231,  0.06900181,  0.1553409 ,\n",
      "        0.0894454 , -0.13426825, -0.06216501,  0.1155136 ,  0.04027628,\n",
      "        0.12180395,  0.04936002,  0.1051098 ,  0.12256032, -0.10105035,\n",
      "       -0.06963648, -0.02305838,  0.07563861, -0.02180905, -0.06258713,\n",
      "       -0.01426108, -0.0129481 ,  0.06690355,  0.13664436, -0.01436797,\n",
      "       -0.1553468 , -0.13724454,  0.14821672,  0.01733512, -0.05041938,\n",
      "       -0.12199777,  0.07615177, -0.05382651, -0.12596524,  0.05441522,\n",
      "        0.1750009 ,  0.11941727,  0.09891243,  0.10104139,  0.1081334 ,\n",
      "       -0.10716577,  0.04232199, -0.03638774,  0.13640866,  0.02086043,\n",
      "        0.11502181,  0.07733084,  0.17596412,  0.1507246 , -0.06950416,\n",
      "        0.01449948, -0.09463827, -0.11388674, -0.1313418 , -0.12561245,\n",
      "        0.12222935, -0.04523734,  0.06121567, -0.10346895,  0.10918163,\n",
      "        0.0883616 , -0.07123069, -0.06584582,  0.08023662, -0.05272616,\n",
      "       -0.12292   ,  0.09063938, -0.07971878,  0.00110074,  0.13585563,\n",
      "       -0.00816803, -0.07476764,  0.0357033 , -0.06657563, -0.08800739,\n",
      "        0.13458803, -0.01732656,  0.13468976,  0.07175318, -0.06277324,\n",
      "        0.0299947 ,  0.13946396, -0.14251146, -0.13216983,  0.04412769,\n",
      "        0.1571706 ,  0.07795945, -0.09560882,  0.10429884, -0.07096502,\n",
      "        0.13238822,  0.01857346, -0.0549546 , -0.05551939,  0.04338595,\n",
      "        0.18760364,  0.21851568, -0.1477787 ,  0.10753598, -0.0585273 ,\n",
      "        0.11226757,  0.18046644,  0.06823143, -0.07704331, -0.13726747,\n",
      "       -0.05315088,  0.06160583, -0.17824131,  0.1422626 ,  0.14083247,\n",
      "        0.14069185,  0.01020405, -0.09793927,  0.06310876, -0.07193034,\n",
      "       -0.06467464, -0.0413811 , -0.1052548 ,  0.12235869, -0.14973971,\n",
      "        0.18263939, -0.08332012, -0.01594763], dtype=float32), array([[-0.00915123, -0.02983172,  0.07144773, ...,  0.10093697,\n",
      "        -0.06065496,  0.01875303],\n",
      "       [ 0.02932422, -0.0467406 ,  0.01882315, ..., -0.06852808,\n",
      "         0.14184758,  0.10760719],\n",
      "       [-0.03294519, -0.00241825, -0.05703708, ...,  0.10917284,\n",
      "        -0.0942468 , -0.01123977],\n",
      "       ...,\n",
      "       [ 0.02672653, -0.1489443 ,  0.04200028, ..., -0.17478262,\n",
      "         0.11071896, -0.02293993],\n",
      "       [-0.21968491, -0.04502407, -0.06405507, ..., -0.05786813,\n",
      "        -0.05784961, -0.08931675],\n",
      "       [-0.12155575,  0.01884755, -0.08993792, ..., -0.15933856,\n",
      "        -0.02418585, -0.12520829]], dtype=float32), array([-0.1794738 , -0.14955129, -0.06815441,  0.06990024, -0.14431939,\n",
      "       -0.18428077,  0.14563997, -0.0862255 ,  0.13828966,  0.00104755,\n",
      "        0.04792587,  0.11522097, -0.01290176,  0.03778905, -0.2560826 ,\n",
      "        0.23433283, -0.22762762, -0.27731997,  0.1592137 ,  0.14134443,\n",
      "        0.03943425, -0.18304981, -0.48377642,  0.15416616,  0.07855853,\n",
      "       -0.08012533, -0.0939096 ,  0.27101147,  0.08107258, -0.24264094,\n",
      "       -0.18007371,  0.01733316,  0.15893403, -0.17712733, -0.11176696,\n",
      "       -0.02214955,  0.0246166 , -0.28800458,  0.19275671, -0.28839582,\n",
      "        0.05985678, -0.10779331, -0.03970247,  0.06666953, -0.01752464,\n",
      "        0.06173213,  0.16166498,  0.02085474, -0.24850148, -0.06847518,\n",
      "        0.15119553, -0.10011268,  0.05694869, -0.25516757, -0.02437978,\n",
      "        0.0455875 , -0.11155725,  0.0895053 , -0.00360454,  0.1610647 ,\n",
      "        0.09361809,  0.02312811,  0.06348152,  0.0200541 ], dtype=float32), array([[ 0.05538689, -0.07307477, -0.16655298, ..., -0.10342126,\n",
      "         0.05467308, -0.03981841],\n",
      "       [ 0.19509055, -0.01743408, -0.12614907, ...,  0.16190636,\n",
      "        -0.04309236, -0.01846827],\n",
      "       [-0.03438425, -0.00835214,  0.15563814, ...,  0.18932258,\n",
      "        -0.0079349 , -0.19235344],\n",
      "       ...,\n",
      "       [-0.02412653,  0.01021927,  0.32200876, ...,  0.17683023,\n",
      "         0.01411922, -0.06260533],\n",
      "       [-0.01663531, -0.05017408,  0.06200607, ..., -0.20421714,\n",
      "         0.02193155,  0.11380744],\n",
      "       [-0.00503355, -0.02571312,  0.12914594, ...,  0.05457466,\n",
      "        -0.0335919 , -0.16767322]], dtype=float32), array([-0.17632443,  0.20911907,  0.69314665,  0.7500646 ,  0.11124047,\n",
      "        0.27024275,  0.56509244,  0.14203621, -0.34601736,  0.674957  ,\n",
      "        0.0264835 ,  0.0742425 ,  0.04988023,  0.18283682, -0.09207342,\n",
      "        0.07319427,  0.5855064 ,  0.37085134, -0.19290909, -0.22016604,\n",
      "       -0.27226102,  0.15391289,  0.02136457,  0.05037043, -0.30828604,\n",
      "        0.15565652,  0.5630625 , -0.05613079,  0.05170351,  0.36205763,\n",
      "       -0.15103346,  0.09930515], dtype=float32), array([[ 0.14904694, -0.20668843],\n",
      "       [-0.16251189,  0.21492477],\n",
      "       [-0.07939807, -0.03440553],\n",
      "       [-0.06928129, -0.03094581],\n",
      "       [-0.16375597,  0.19387107],\n",
      "       [ 0.44687477,  0.2550321 ],\n",
      "       [-0.08481392, -0.04146806],\n",
      "       [-0.15608743,  0.18203902],\n",
      "       [-0.12402682,  0.19270922],\n",
      "       [-0.09044636, -0.05237482],\n",
      "       [-0.25351626,  0.3161526 ],\n",
      "       [-0.19235687,  0.24518669],\n",
      "       [-0.17227471,  0.21484843],\n",
      "       [ 0.34322202,  0.19458093],\n",
      "       [ 0.10779184, -0.14827077],\n",
      "       [-0.21742678,  0.2826278 ],\n",
      "       [-0.09449787, -0.04942056],\n",
      "       [-0.13339807, -0.06747404],\n",
      "       [ 0.4282956 ,  0.23387398],\n",
      "       [ 0.174478  , -0.23603375],\n",
      "       [-0.16852655,  0.22591731],\n",
      "       [ 0.3260491 ,  0.1803061 ],\n",
      "       [ 0.4191894 ,  0.24117588],\n",
      "       [ 0.46074057,  0.2589947 ],\n",
      "       [ 0.25090274, -0.3474921 ],\n",
      "       [ 0.2872455 ,  0.16749115],\n",
      "       [-0.10512309, -0.05152068],\n",
      "       [-0.27382994, -0.14062433],\n",
      "       [-0.1834469 ,  0.2343792 ],\n",
      "       [-0.12778348, -0.06462023],\n",
      "       [-0.1969118 ,  0.27331123],\n",
      "       [ 0.42483625,  0.23887633]], dtype=float32), array([-0.0602901 ,  0.04848797], dtype=float32), array([[-0.0430959 , -0.14103772],\n",
      "       [-0.05121933, -0.07321887],\n",
      "       [-0.3965874 , -0.3485917 ],\n",
      "       [-0.52726364, -0.62474   ],\n",
      "       [-0.05274985, -0.0222757 ],\n",
      "       [-0.16103514, -0.22146605],\n",
      "       [-0.21866515, -0.15743525],\n",
      "       [-0.05100448, -0.02087129],\n",
      "       [ 0.21727075,  0.24567315],\n",
      "       [-0.26898184, -0.42413524],\n",
      "       [ 0.02997   , -0.05990509],\n",
      "       [-0.04155676, -0.06135632],\n",
      "       [-0.00731107, -0.00826238],\n",
      "       [-0.11815584, -0.07927579],\n",
      "       [-0.10874771, -0.07648827],\n",
      "       [-0.08378644,  0.04087605],\n",
      "       [-0.11722165, -0.2939535 ],\n",
      "       [-0.08360282, -0.18025221],\n",
      "       [-0.01972126, -0.01808144],\n",
      "       [-0.04687248, -0.15951549],\n",
      "       [ 0.10887248,  0.08038694],\n",
      "       [-0.04522223, -0.07835574],\n",
      "       [-0.0293634 , -0.09862319],\n",
      "       [-0.00119416, -0.07105132],\n",
      "       [ 0.05498068, -0.15494476],\n",
      "       [-0.06582248, -0.0566978 ],\n",
      "       [-0.19851145, -0.24125883],\n",
      "       [-0.00480099, -0.13343492],\n",
      "       [-0.05910065, -0.02771216],\n",
      "       [-0.2675429 , -0.20876138],\n",
      "       [ 0.18886998,  0.03958318],\n",
      "       [-0.08047308, -0.05461239]], dtype=float32), array([-0.9923035, -1.0876162], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01942308, -0.01637648,  0.04963167, ...,  0.05502933,\n",
      "         0.02832877, -0.02377152],\n",
      "       [ 0.00511583, -0.0095196 , -0.03313178, ...,  0.03536819,\n",
      "        -0.04230474,  0.04265615],\n",
      "       [ 0.03133994,  0.03281501,  0.04295759, ...,  0.02680593,\n",
      "         0.07079177, -0.02597428],\n",
      "       ...,\n",
      "       [ 0.02833071, -0.01984979,  0.04468694, ...,  0.00930388,\n",
      "         0.00944376,  0.06359959],\n",
      "       [ 0.05768855,  0.05408427,  0.02434234, ...,  0.07189935,\n",
      "        -0.01539868,  0.01984672],\n",
      "       [-0.05383008, -0.04783966,  0.05436985, ...,  0.03914557,\n",
      "        -0.03687038,  0.07381545]], dtype=float32), array([ 0.0809879 , -0.14468764,  0.20034534, -0.07052892, -0.10295361,\n",
      "        0.07901033,  0.1504663 ,  0.12362251, -0.00227833, -0.17386064,\n",
      "        0.11071022,  0.07361724, -0.00439805,  0.09749707,  0.00434813,\n",
      "        0.12714131, -0.15235776,  0.12832388,  0.0929666 ,  0.06856047,\n",
      "        0.06873886, -0.17188625,  0.08900192,  0.1415954 ,  0.1600587 ,\n",
      "       -0.20469408,  0.09264118,  0.1407531 , -0.01869877,  0.15862823,\n",
      "       -0.13908228,  0.06697091,  0.12093801,  0.16051778,  0.16225806,\n",
      "       -0.14698122,  0.02452355,  0.15866081,  0.20576778,  0.15522468,\n",
      "        0.18181664,  0.04572291,  0.06434864,  0.08899505,  0.03720754,\n",
      "       -0.10365952,  0.06958623, -0.16288766,  0.21886435, -0.06488471,\n",
      "        0.14199424,  0.07883952,  0.14359163,  0.18577771,  0.10375676,\n",
      "        0.11546788,  0.02622037,  0.15559533, -0.09695808,  0.14401184,\n",
      "        0.17426498,  0.03180396,  0.12706783, -0.10007996,  0.09553278,\n",
      "        0.20742173,  0.14278573,  0.18994889,  0.08858737,  0.12294316,\n",
      "        0.15820526, -0.134687  ,  0.09396578, -0.13203366,  0.16539021,\n",
      "        0.09567118, -0.10957405,  0.12578385, -0.06991654,  0.18452507,\n",
      "       -0.12478929, -0.21869259,  0.11468932,  0.23035917,  0.13500546,\n",
      "        0.00916489, -0.19193958, -0.04801618,  0.02245501, -0.08010103,\n",
      "        0.19765693,  0.1591393 ,  0.18279034, -0.20830847, -0.14263421,\n",
      "       -0.15763009, -0.0805143 , -0.09933479, -0.10188255, -0.05607761,\n",
      "        0.02785588, -0.10283293,  0.08817464,  0.1850892 ,  0.15319793,\n",
      "        0.14997722,  0.12828219, -0.12335693,  0.02801969, -0.15539348,\n",
      "        0.06221382,  0.15193346,  0.19120961,  0.19292557,  0.07390349,\n",
      "        0.11637449, -0.05395938,  0.11419588,  0.20058948,  0.22357237,\n",
      "       -0.13242787, -0.05659147,  0.20230153,  0.04621612,  0.11657062,\n",
      "        0.04865874, -0.11669654,  0.01790973,  0.1762322 , -0.14432754,\n",
      "        0.13704106,  0.10255907, -0.00440084,  0.1406111 , -0.0972498 ,\n",
      "       -0.04308583,  0.19025397,  0.13464642,  0.14283861,  0.07475797,\n",
      "        0.17356588, -0.00810552,  0.15331073,  0.1331159 ,  0.13685434,\n",
      "       -0.00559373,  0.17427044,  0.1853891 ,  0.24533474,  0.10942312,\n",
      "        0.06269439,  0.19283983,  0.06806761, -0.00260751,  0.05484315,\n",
      "        0.0924331 ,  0.06174448,  0.15537627,  0.17913316,  0.11310809,\n",
      "       -0.17017297,  0.21882291,  0.17901467,  0.06452285,  0.21683198,\n",
      "        0.19685043, -0.08013786,  0.1704226 ,  0.15194169,  0.18483734,\n",
      "        0.00424176,  0.13057935,  0.07403537,  0.135839  ,  0.17965002,\n",
      "        0.0745398 ,  0.06153892,  0.21277258,  0.09947878,  0.12286649,\n",
      "        0.24482499,  0.21209809,  0.13515373,  0.11929002,  0.10278001,\n",
      "        0.15475631,  0.171783  ,  0.15814622,  0.20199338, -0.17116903,\n",
      "       -0.0170741 ,  0.16781066,  0.1044841 , -0.0874851 ,  0.17719328,\n",
      "        0.16953334,  0.2233981 ,  0.20059624,  0.10290436,  0.13695157,\n",
      "        0.13400541,  0.04858094, -0.02686938,  0.1485469 ,  0.21477208,\n",
      "       -0.09945066,  0.19771275,  0.1834268 ,  0.03462965, -0.18109202,\n",
      "        0.10349889,  0.16513793,  0.09551168,  0.12026334,  0.01431079,\n",
      "       -0.11719636, -0.12422215,  0.162245  ,  0.12877147,  0.15450783,\n",
      "        0.1049328 ,  0.1329532 ,  0.12404594,  0.08096038,  0.12161792,\n",
      "        0.08495329, -0.0220954 ,  0.06670102,  0.18113375, -0.15877031,\n",
      "       -0.13957427,  0.10668248,  0.12903504,  0.0752221 ,  0.07009336,\n",
      "        0.20600763,  0.11716902,  0.12671784,  0.21662295,  0.11869443,\n",
      "        0.09129737, -0.02869326,  0.21123789,  0.05747156,  0.19189328,\n",
      "       -0.0763365 ,  0.0890359 ,  0.17052042,  0.10754702, -0.03733861,\n",
      "       -0.10911558,  0.13813208,  0.07117286,  0.06279117,  0.08545966,\n",
      "       -0.13219215], dtype=float32), array([[ 0.08240114,  0.02658531,  0.06106296, ..., -0.07074529,\n",
      "        -0.06774506, -0.00848183],\n",
      "       [ 0.01046252, -0.02184125, -0.07821654, ...,  0.02979861,\n",
      "         0.02527386,  0.02030041],\n",
      "       [-0.0210644 ,  0.07629184,  0.00953364, ...,  0.08275633,\n",
      "        -0.05440843,  0.08518402],\n",
      "       ...,\n",
      "       [ 0.03294419,  0.07409322,  0.09609896, ..., -0.11029425,\n",
      "        -0.02540564,  0.06935254],\n",
      "       [ 0.11610712, -0.02689479, -0.01977878, ...,  0.05817182,\n",
      "         0.00822531, -0.00912326],\n",
      "       [-0.03210009, -0.02309664, -0.06972276, ...,  0.00359596,\n",
      "         0.11272468, -0.00888189]], dtype=float32), array([ 0.10254303, -0.01703299,  0.02378203,  0.00895111, -0.08684514,\n",
      "        0.12899864, -0.05706243,  0.05161078,  0.06957901,  0.15245531,\n",
      "        0.07646744, -0.13658004, -0.07464276,  0.10539636,  0.02794832,\n",
      "        0.10754458,  0.04349761,  0.10599074,  0.10468686, -0.10423347,\n",
      "       -0.0675006 , -0.03067065,  0.06140695, -0.02870833, -0.06198771,\n",
      "       -0.02849702, -0.01621631,  0.0577116 ,  0.13633782, -0.01608684,\n",
      "       -0.1483386 , -0.13609467,  0.13304448,  0.00929766, -0.06384842,\n",
      "       -0.13153435,  0.09103791, -0.05551428, -0.1174757 ,  0.03695887,\n",
      "        0.16477928,  0.11873968,  0.10237647,  0.08535579,  0.09991892,\n",
      "       -0.11274728,  0.04465801, -0.05051122,  0.1308363 ,  0.01858949,\n",
      "        0.10146267,  0.0766118 ,  0.16759509,  0.1357327 , -0.08344955,\n",
      "        0.00562828, -0.10277932, -0.10799288, -0.13189265, -0.11071447,\n",
      "        0.11876059, -0.05878354,  0.05693838, -0.09747506,  0.10219612,\n",
      "        0.08011221, -0.07703763, -0.07071133,  0.08083972, -0.06598585,\n",
      "       -0.12495743,  0.08255302, -0.0920964 ,  0.00119936,  0.13126113,\n",
      "        0.00852758, -0.08057878,  0.02680709, -0.08420172, -0.08225799,\n",
      "        0.11802002, -0.01915815,  0.131208  ,  0.06399316, -0.05422856,\n",
      "        0.03583386,  0.128886  , -0.1400075 , -0.1352625 ,  0.03440559,\n",
      "        0.16249791,  0.07177535, -0.09790077,  0.09251359, -0.09076352,\n",
      "        0.12180415,  0.02596505, -0.05303617, -0.06064847,  0.03034618,\n",
      "        0.18389584,  0.20675269, -0.14841437,  0.08986141, -0.05846619,\n",
      "        0.11848849,  0.17295425,  0.05681999, -0.06534013, -0.14176992,\n",
      "       -0.05896566,  0.05502196, -0.18183747,  0.1434585 ,  0.12036288,\n",
      "        0.13407898,  0.01317232, -0.1041991 ,  0.06394953, -0.07799554,\n",
      "       -0.08484894, -0.05226805, -0.10115353,  0.11672258, -0.1514138 ,\n",
      "        0.176175  , -0.08837107, -0.01620407], dtype=float32), array([[-0.00346221, -0.05354858,  0.06868287, ...,  0.09562706,\n",
      "        -0.05595652,  0.00240477],\n",
      "       [ 0.01560535, -0.04726258,  0.0038498 , ..., -0.0908583 ,\n",
      "         0.16442664,  0.08349752],\n",
      "       [-0.03518542, -0.02076817, -0.05348855, ...,  0.11050933,\n",
      "        -0.09409019, -0.03673713],\n",
      "       ...,\n",
      "       [ 0.00574106, -0.14147022,  0.04568367, ..., -0.1762982 ,\n",
      "         0.11482021, -0.01762323],\n",
      "       [-0.2145655 , -0.03921192, -0.04667826, ..., -0.05067893,\n",
      "        -0.07405062, -0.08489591],\n",
      "       [-0.12710291,  0.01337373, -0.08737206, ..., -0.16155502,\n",
      "        -0.02954014, -0.1210572 ]], dtype=float32), array([-0.18203048, -0.14943907, -0.08034159,  0.07179119, -0.12907925,\n",
      "       -0.19111452,  0.13667215, -0.11077637,  0.13211347, -0.01891811,\n",
      "        0.04881639,  0.1071934 , -0.02720027,  0.03174515, -0.2579489 ,\n",
      "        0.25010574, -0.22768901, -0.27339187,  0.13834584,  0.12852293,\n",
      "        0.01730073, -0.18964203, -0.4641885 ,  0.1576713 ,  0.08707366,\n",
      "       -0.05670375, -0.08607544,  0.25946066,  0.07983966, -0.2521456 ,\n",
      "       -0.17303579,  0.00669797,  0.13750762, -0.17681742, -0.11082336,\n",
      "       -0.01947429,  0.03221909, -0.27488983,  0.1939466 , -0.27825087,\n",
      "        0.06441625, -0.11261781, -0.03530555,  0.04969161, -0.03764707,\n",
      "        0.07259378,  0.1523847 ,  0.01869985, -0.23195355, -0.07332255,\n",
      "        0.13377768, -0.10528377,  0.04597467, -0.23883687, -0.02734768,\n",
      "        0.04244116, -0.12742616,  0.08241575, -0.01067504,  0.15343462,\n",
      "        0.09655778,  0.01796119,  0.06723566, -0.00419782], dtype=float32), array([[ 0.07337634, -0.09070306, -0.17420757, ..., -0.09670132,\n",
      "         0.07395109, -0.04161596],\n",
      "       [ 0.20128715, -0.01409128, -0.12499889, ...,  0.16729018,\n",
      "        -0.04765143,  0.00559014],\n",
      "       [-0.05743797, -0.01242064,  0.14142056, ...,  0.18831739,\n",
      "         0.00556772, -0.19522394],\n",
      "       ...,\n",
      "       [-0.01607012, -0.00796457,  0.32125312, ...,  0.18170448,\n",
      "        -0.00863423, -0.0492358 ],\n",
      "       [-0.00613269, -0.06899338,  0.06912354, ..., -0.20381303,\n",
      "         0.02619537,  0.12058705],\n",
      "       [-0.0042347 , -0.03326039,  0.11761244, ...,  0.03895522,\n",
      "        -0.03863908, -0.16550735]], dtype=float32), array([-0.13821775,  0.20862883,  0.66344374,  0.7262304 ,  0.11466084,\n",
      "        0.25705603,  0.55495554,  0.12255957, -0.34120533,  0.66354966,\n",
      "        0.01450077,  0.06606209,  0.03683309,  0.18868285, -0.07578196,\n",
      "        0.04818044,  0.5722975 ,  0.38018   , -0.22021195, -0.19145668,\n",
      "       -0.27832648,  0.15784775,  0.02308918,  0.05250821, -0.2581783 ,\n",
      "        0.16142766,  0.54478526, -0.0337316 ,  0.04441382,  0.3475903 ,\n",
      "       -0.15158683,  0.09861238], dtype=float32), array([[ 0.15135014, -0.20696135],\n",
      "       [-0.16249393,  0.2052419 ],\n",
      "       [-0.07333561, -0.0335392 ],\n",
      "       [-0.06585123, -0.03307334],\n",
      "       [-0.13855904,  0.19607407],\n",
      "       [ 0.44982827,  0.25366858],\n",
      "       [-0.09481678, -0.04656913],\n",
      "       [-0.1554542 ,  0.18551846],\n",
      "       [-0.10839367,  0.18032664],\n",
      "       [-0.08729953, -0.04593625],\n",
      "       [-0.26444325,  0.31385353],\n",
      "       [-0.19852565,  0.26048774],\n",
      "       [-0.16817914,  0.21949935],\n",
      "       [ 0.3454652 ,  0.2061431 ],\n",
      "       [ 0.10535795, -0.15201785],\n",
      "       [-0.23072848,  0.28426847],\n",
      "       [-0.09461398, -0.04592174],\n",
      "       [-0.14016362, -0.06966622],\n",
      "       [ 0.443177  ,  0.23667131],\n",
      "       [ 0.17655726, -0.2391288 ],\n",
      "       [-0.17439573,  0.22571833],\n",
      "       [ 0.3277291 ,  0.18720818],\n",
      "       [ 0.43478918,  0.24359088],\n",
      "       [ 0.47395197,  0.25879467],\n",
      "       [ 0.2709037 , -0.34545368],\n",
      "       [ 0.29309377,  0.17302433],\n",
      "       [-0.09571339, -0.05268678],\n",
      "       [-0.28044754, -0.14790186],\n",
      "       [-0.18949187,  0.24653363],\n",
      "       [-0.12063462, -0.06954401],\n",
      "       [-0.2044326 ,  0.26336226],\n",
      "       [ 0.4393303 ,  0.24015805]], dtype=float32), array([-0.05751806,  0.04807793], dtype=float32), array([[-4.22524624e-02, -1.54355586e-01],\n",
      "       [-6.74901828e-02, -9.21165422e-02],\n",
      "       [-3.98079008e-01, -3.45389307e-01],\n",
      "       [-5.34042656e-01, -6.24858916e-01],\n",
      "       [-6.38413057e-02, -2.28541009e-02],\n",
      "       [-1.64029986e-01, -2.20269948e-01],\n",
      "       [-2.44441777e-01, -1.54919624e-01],\n",
      "       [-6.00179732e-02, -3.14497165e-02],\n",
      "       [ 2.45198548e-01,  2.70850271e-01],\n",
      "       [-2.76564658e-01, -4.39242691e-01],\n",
      "       [ 3.32388096e-02, -5.78776300e-02],\n",
      "       [-4.79640029e-02, -7.99862370e-02],\n",
      "       [-1.25657879e-02, -9.31342971e-03],\n",
      "       [-1.14647448e-01, -8.24646875e-02],\n",
      "       [-1.31803989e-01, -8.51340294e-02],\n",
      "       [-1.03865385e-01,  5.59201129e-02],\n",
      "       [-1.27474010e-01, -3.06827456e-01],\n",
      "       [-7.94536322e-02, -2.04144821e-01],\n",
      "       [-3.38399899e-03,  3.64029635e-04],\n",
      "       [-4.73293811e-02, -1.71827778e-01],\n",
      "       [ 1.21825144e-01,  9.34927166e-02],\n",
      "       [-4.31534275e-02, -8.14304575e-02],\n",
      "       [-2.21134499e-02, -1.17534310e-01],\n",
      "       [ 8.47146288e-03, -8.92965496e-02],\n",
      "       [ 5.53305857e-02, -1.59295142e-01],\n",
      "       [-5.90038076e-02, -5.89807816e-02],\n",
      "       [-2.03841522e-01, -2.61900544e-01],\n",
      "       [ 1.29615972e-02, -1.56475917e-01],\n",
      "       [-5.00447005e-02, -1.69085786e-02],\n",
      "       [-2.69139260e-01, -2.31403440e-01],\n",
      "       [ 2.03362986e-01,  5.29673696e-02],\n",
      "       [-7.76694193e-02, -3.46202739e-02]], dtype=float32), array([-0.9334521, -1.0324043], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# from time import time\n",
    "# import unittest\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import keras\n",
    "# from sklearn.metrics import silhouette_score\n",
    "\n",
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1' \n",
    "\n",
    "# from keras.datasets import mnist\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# # config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "# set_session(tf.Session(config=config))\n",
    "# from keras.datasets import mnist\n",
    "# import numpy as np\n",
    "# (xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "# xTrain = xTrain.astype('float32') / 255.\n",
    "# xTest = xTest.astype('float32') / 255.\n",
    "# numTrain = len(xTrain)\n",
    "# numTest = len(xTest)\n",
    "# sizeDigit = xTrain.shape[1:]\n",
    "\n",
    "# dimInput = np.prod(xTrain.shape[1:])\n",
    "# xTrain = xTrain.reshape((numTrain, dimInput))\n",
    "# xTest = xTest.reshape((numTest, dimInput))\n",
    "# print(xTrain.shape)\n",
    "# print(xTest.shape)\n",
    "\n",
    "# numEpochs = 200\n",
    "# sizeBatch = 1024\n",
    "# sizeKernel = 3\n",
    "# layerDense = [256, 128, 64, 32, 2]\n",
    "# ratRecon = 1\n",
    "# nameOptim = 'adam'\n",
    "# pathTempBest = '../model/temp/'\n",
    "# patience = 2\n",
    "# stdEps = 1.0\n",
    "\n",
    "# vae = VAE(dimInput, layerDense=layerDense, ratRecon=ratRecon)\n",
    "# history, timeTrain = vae.fit(xTrain, xTest, \n",
    "#                              numEpochs=numEpochs,\n",
    "#                              sizeBatch=sizeBatch,\n",
    "#                              pathTempBest=pathTempBest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
