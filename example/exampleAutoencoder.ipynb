{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ../module/autoencoder.ipynb\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import unittest\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1' \n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import sys  \n",
    "sys.path.append('../')\n",
    "from util.util import *\n",
    "from util import importNotebook\n",
    "from module.autoencoder import VAE, ConvVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Convolutional VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.3004 - val_loss: 0.2424\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.24236, saving model to ../model/temp/Conv_AutoEncoder.01-0.30-0.24.hdf5\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.2302 - val_loss: 0.2202\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.24236 to 0.22017, saving model to ../model/temp/Conv_AutoEncoder.02-0.23-0.22.hdf5\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.2163 - val_loss: 0.2126\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22017 to 0.21259, saving model to ../model/temp/Conv_AutoEncoder.03-0.22-0.21.hdf5\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.2099 - val_loss: 0.2075\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.21259 to 0.20748, saving model to ../model/temp/Conv_AutoEncoder.04-0.21-0.21.hdf5\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 56us/step - loss: 0.2062 - val_loss: 0.2053\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.20748 to 0.20528, saving model to ../model/temp/Conv_AutoEncoder.05-0.21-0.21.hdf5\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.2040 - val_loss: 0.2030\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.20528 to 0.20297, saving model to ../model/temp/Conv_AutoEncoder.06-0.20-0.20.hdf5\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2023 - val_loss: 0.2017\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20297 to 0.20175, saving model to ../model/temp/Conv_AutoEncoder.07-0.20-0.20.hdf5\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.2009 - val_loss: 0.2010\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.20175 to 0.20096, saving model to ../model/temp/Conv_AutoEncoder.08-0.20-0.20.hdf5\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1998 - val_loss: 0.1994\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.20096 to 0.19940, saving model to ../model/temp/Conv_AutoEncoder.09-0.20-0.20.hdf5\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 0.1988 - val_loss: 0.1986\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.19940 to 0.19860, saving model to ../model/temp/Conv_AutoEncoder.10-0.20-0.20.hdf5\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 14, 4)    40          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 7, 16)     592         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 784)          0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           12560       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            34          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            34          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 13,260\n",
      "Trainable params: 13,260\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 784)               13328     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 14, 16)        2320      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 4)         580       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 16,313\n",
      "Trainable params: 16,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc948b88c50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset\n",
    "(xTrain, yTrain), (xTest, yTest) = mnist.load_data()\n",
    "xTrain = xTrain.astype('float32') / 255.\n",
    "xTest = xTest.astype('float32') / 255.\n",
    "numTrain = len(xTrain)\n",
    "numTest = len(xTest)\n",
    "sizeDigit = xTrain.shape[1:]\n",
    "dimInput = [*xTrain.shape[1:], 1]\n",
    "xTrain = xTrain.reshape((numTrain, *dimInput))\n",
    "xTest = xTest.reshape((numTest, *dimInput))\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n",
    "\n",
    "# Set parameters\n",
    "numEpochs = 10\n",
    "sizeBatch = 128\n",
    "sizeKernel = 3\n",
    "layerDense = [16, 2]\n",
    "layerConv = [4, 16]\n",
    "ratRecon = 1\n",
    "nameOptim = 'adam'\n",
    "modelPath = '../model/temp/'\n",
    "patience = 3\n",
    "\n",
    "# Initialize and train\n",
    "convVAE = ConvVAE(dimInput, layerDense=layerDense, layerConv=layerConv, ratRecon=ratRecon)\n",
    "history, timeTrain = convVAE.fit(xTrain, xTest, \n",
    "                                 numEpochs=numEpochs,\n",
    "                                 sizeBatch=sizeBatch,\n",
    "                                 tempPathBest=modelPath)\n",
    "\n",
    "# Get the encoder and decoder \n",
    "encoder = convVAE.encoder\n",
    "decoder = convVAE.decoder\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "\n",
    "# Encode testing dataset and get generated reconstruction\n",
    "encode = encoder.predict(xTest)\n",
    "generate = decoder.predict(np.array([[0, 0]]))\n",
    "plt.imshow(generate.reshape(sizeDigit))   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
