{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\anaconda3\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ..\\util\\dataProcess.ipynb\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "numSeed = 42\n",
    "np.random.seed(numSeed)\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Lambda, Conv2D, Conv2DTranspose, Activation, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import mnist\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import sys  \n",
    "sys.path.append('../')\n",
    "from loadFrey import *\n",
    "from util import importNotebook\n",
    "from util.dataProcess import *\n",
    "from util.util import plotScatterDecode, plotProgress, plotCompDecode, plotScatterEncode, addNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 50\n",
    "sizeBatch = 32\n",
    "sizeKernel = 3\n",
    "dimInter = 64\n",
    "dimEncode = 2\n",
    "layer_filters = [16, 32]\n",
    "stdEps = 1.0 \n",
    "ratRecon = 0.998\n",
    "factNoise = 0\n",
    "nameOptim = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1965, 28, 20, 1)\n",
      "(1965, 28, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "data = load_frey_face_images('frey_rawface.mat')\n",
    "xTrain, xTest, __ = splitData(data, ratio=[1, 0])\n",
    "xTrain = xTrain.astype('float32') / 255.\n",
    "xTest = xTrain\n",
    "numTrain = len(xTrain)\n",
    "numTest = len(xTest)\n",
    "dimInput = [*xTrain.shape[1:], 1]  # adapt 28*20 as 28*20*1\n",
    "\n",
    "xTrain = np.reshape(xTrain, [-1, *dimInput])\n",
    "xTest = np.reshape(xTest, [-1, *dimInput])\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 28, 20, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 14, 10, 16)   160         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 7, 5, 32)     4640        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1120)         0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           71744       flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            130         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            130         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           dense_2[0][0]                    \n",
      "                                                                 dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 76,804\n",
      "Trainable params: 76,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1120)              72800     \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 7, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 14, 10, 32)        9248      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 20, 16)        4624      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 28, 20, 1)         145       \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 28, 20, 1)         0         \n",
      "=================================================================\n",
      "Total params: 87,009\n",
      "Trainable params: 87,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(dimInput))  # adapt this if using `channels_first` image data format\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=sizeKernel,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "x = Dense(dimInter, activation='relu')(x)\n",
    "zMean = Dense(dimEncode)(x)\n",
    "zSigmaLog = Dense(dimEncode)(x) # log for linear dense\n",
    "\n",
    "def sampling(args):\n",
    "    zMean, zSigmaLog = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(zMean)[0], dimEncode),\n",
    "                              mean=0., stddev=stdEps)\n",
    "    return zMean + K.exp(zSigmaLog) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "z = Lambda(sampling)([zMean, zSigmaLog])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Build the Decoder Model\n",
    "inputLatent = Input(shape=(dimEncode,), name='decoder_input')\n",
    "x = Dense(dimInter, activation='relu')(inputLatent)\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(x)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=sizeKernel,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=1,\n",
    "                    kernel_size=sizeKernel,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(inputLatent, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 20, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 2)                 76804     \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 28, 20, 1)         87009     \n",
      "=================================================================\n",
      "Total params: 163,813\n",
      "Trainable params: 163,813\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1965 samples, validate on 1965 samples\n",
      "Epoch 1/50\n",
      "1965/1965 [==============================] - 3s 2ms/step - loss: 0.2660 - val_loss: 0.0315\n",
      "Epoch 2/50\n",
      "1965/1965 [==============================] - 1s 615us/step - loss: 0.0266 - val_loss: 0.0230\n",
      "Epoch 3/50\n",
      "1965/1965 [==============================] - 1s 604us/step - loss: 0.0215 - val_loss: 0.0204\n",
      "Epoch 4/50\n",
      "1965/1965 [==============================] - 1s 629us/step - loss: 0.0198 - val_loss: 0.0193\n",
      "Epoch 5/50\n",
      "1965/1965 [==============================] - 1s 601us/step - loss: 0.0189 - val_loss: 0.0186\n",
      "Epoch 6/50\n",
      "1965/1965 [==============================] - 1s 617us/step - loss: 0.0184 - val_loss: 0.0182\n",
      "Epoch 7/50\n",
      "1965/1965 [==============================] - 1s 604us/step - loss: 0.0180 - val_loss: 0.0179\n",
      "Epoch 8/50\n",
      "1965/1965 [==============================] - 1s 635us/step - loss: 0.0177 - val_loss: 0.0176\n",
      "Epoch 9/50\n",
      "1965/1965 [==============================] - 1s 545us/step - loss: 0.0174 - val_loss: 0.0174\n",
      "Epoch 10/50\n",
      "1965/1965 [==============================] - 1s 607us/step - loss: 0.0173 - val_loss: 0.0172\n",
      "Epoch 11/50\n",
      "1965/1965 [==============================] - 1s 618us/step - loss: 0.0171 - val_loss: 0.0171\n",
      "Epoch 12/50\n",
      "1965/1965 [==============================] - 1s 588us/step - loss: 0.0170 - val_loss: 0.0170\n",
      "Epoch 13/50\n",
      "1965/1965 [==============================] - 1s 614us/step - loss: 0.0169 - val_loss: 0.0169\n",
      "Epoch 14/50\n",
      "1965/1965 [==============================] - 1s 596us/step - loss: 0.0169 - val_loss: 0.0168\n",
      "Epoch 15/50\n",
      "1965/1965 [==============================] - 1s 633us/step - loss: 0.0168 - val_loss: 0.0168\n",
      "Epoch 16/50\n",
      "1965/1965 [==============================] - 1s 638us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 17/50\n",
      "1965/1965 [==============================] - 1s 599us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 18/50\n",
      "1965/1965 [==============================] - 1s 590us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 19/50\n",
      "1965/1965 [==============================] - 1s 607us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 20/50\n",
      "1965/1965 [==============================] - 1s 628us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 21/50\n",
      "1965/1965 [==============================] - 1s 627us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 22/50\n",
      "1965/1965 [==============================] - 1s 611us/step - loss: 0.0167 - val_loss: 0.0167\n",
      "Epoch 23/50\n",
      "1965/1965 [==============================] - 1s 670us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 24/50\n",
      "1965/1965 [==============================] - 1s 635us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 25/50\n",
      "1965/1965 [==============================] - 1s 623us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 26/50\n",
      "1965/1965 [==============================] - 1s 637us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 27/50\n",
      "1965/1965 [==============================] - 1s 624us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 28/50\n",
      "1965/1965 [==============================] - 1s 629us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 29/50\n",
      "1965/1965 [==============================] - 1s 608us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 30/50\n",
      "1965/1965 [==============================] - 1s 638us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 31/50\n",
      "1965/1965 [==============================] - 1s 621us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 32/50\n",
      "1965/1965 [==============================] - 1s 628us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 33/50\n",
      "1965/1965 [==============================] - 1s 628us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 34/50\n",
      "1965/1965 [==============================] - 1s 641us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 35/50\n",
      "1965/1965 [==============================] - 1s 531us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 36/50\n",
      "1965/1965 [==============================] - 1s 549us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 37/50\n",
      "1965/1965 [==============================] - 1s 616us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 38/50\n",
      "1965/1965 [==============================] - 1s 729us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 39/50\n",
      "1965/1965 [==============================] - 1s 644us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 40/50\n",
      "1965/1965 [==============================] - 1s 632us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 41/50\n",
      "1965/1965 [==============================] - 1s 662us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 42/50\n",
      "1965/1965 [==============================] - 1s 628us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 43/50\n",
      "1965/1965 [==============================] - 1s 609us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 44/50\n",
      "1965/1965 [==============================] - 1s 657us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 45/50\n",
      "1965/1965 [==============================] - 1s 679us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 46/50\n",
      "1965/1965 [==============================] - 1s 654us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 47/50\n",
      "1965/1965 [==============================] - 1s 664us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 48/50\n",
      "1965/1965 [==============================] - 1s 589us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 49/50\n",
      "1965/1965 [==============================] - 1s 662us/step - loss: 0.0166 - val_loss: 0.0166\n",
      "Epoch 50/50\n",
      "1965/1965 [==============================] - 1s 614us/step - loss: 0.0166 - val_loss: 0.0166\n"
     ]
    }
   ],
   "source": [
    "def lossVAE(zMean, zSigmaLog):\n",
    "    def loss(tensorInput, tensorDecode):\n",
    "        lossRecon =  metrics.binary_crossentropy(K.flatten(tensorInput), K.flatten(tensorDecode))\n",
    "        lossKL = - 0.5 * K.sum(1 + 2 * zSigmaLog - K.square(zMean) - K.square(K.exp(zSigmaLog)), axis=-1)\n",
    "#         lossKL = - 0.5 * K.mean(1 + zSigmaLog - K.square(zMean) - K.exp(zSigmaLog), axis=-1)\n",
    "        return ratRecon * lossRecon + (1 - ratRecon) * lossKL\n",
    "    return loss\n",
    "\n",
    "autoencoder.compile(optimizer=nameOptim, loss=lossVAE(zMean, zSigmaLog))\n",
    "\n",
    "# Train the autoencoder\n",
    "tic = time()\n",
    "history = autoencoder.fit(xTrain, xTrain,\n",
    "                epochs=numEpochs,\n",
    "                batch_size=sizeBatch,\n",
    "                shuffle=True,\n",
    "                validation_data=(xTest, xTest))\n",
    "timeTrain = time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the historical training progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"traing consumed: \" + str(timeTrain) + \" seconds\")\n",
    "plotProgress(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the encoding and decoding results of testing data, and get the mean/std of the encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encodeTest = encoder.predict(xTestNoise)\n",
    "decodeTest = decoder.predict(encodeTest)\n",
    "meanEncTest = np.mean(encodeTest, axis=0)\n",
    "stdEncTest = np.std(encodeTest, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original digitals with the decoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCompDecode(xTest, decodeTest, xNoise=xTestNoise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the scatter of the encoding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlim = (meanEncTest[0] - 4*stdEncTest[0], meanEncTest[0] + 4*stdEncTest[0])\n",
    "ylim = (meanEncTest[1] - 4*stdEncTest[1], meanEncTest[1] + 4*stdEncTest[1])\n",
    "\n",
    "plotScatterEncode(encodeTest, yTest, xlim, ylim, numShow=10000)\n",
    "scoreSilh = silhouette_score(encodeTest, yTest)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decoding results from the encoding scatter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "plotScatterDecode(decoder, sizeDigit, xlim, ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeTrain, history.history[\"loss\"][numEpochs-1], history.history[\"val_loss\"][numEpochs-1], scoreSilh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, _), (xTest, yTest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
