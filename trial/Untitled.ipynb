{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b2338c269a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1' \n",
    "from time import time\n",
    "import numpy as np\n",
    "numSeed = 42\n",
    "np.random.seed(numSeed)\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras import metrics, backend as K\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import sys  \n",
    "sys.path.append('../')\n",
    "from util.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = list()\n",
    "labels = list()\n",
    "\n",
    "images = os.path.join(\"img_align_celeba_png/\")\n",
    "annot = os.path.join(\"list_attr_celeba_png.txt\")\n",
    "\n",
    "with open(annot) as in_file:\n",
    "    count_datapoints = int(in_file.readline())\n",
    "    plaintext_labels = in_file.readline().split()\n",
    "\n",
    "    for line in in_file:\n",
    "        splitted = line.split()\n",
    "\n",
    "        filenames.append(os.path.join(images, splitted[0]))\n",
    "\n",
    "        properties_celebrity = [float(x) for x in splitted[1:]]\n",
    "        properties_celebrity = [max(0.0, x) for x in properties_celebrity]\n",
    "        labels.append(properties_celebrity)\n",
    "assert len(filenames) == len(labels)\n",
    "\n",
    "# print(labels[:4])\n",
    "# print(filenames[:4])\n",
    "\n",
    "show_number = len(filenames)\n",
    "datasetlist = []\n",
    "# print(plaintext_labels)\n",
    "for filename, properties in zip(filenames[:show_number], labels[:show_number]):\n",
    "    image = Image.open(os.path.join(filename))\n",
    "    image = image.resize([64, 64], Image.ANTIALIAS)\n",
    "    image = (np.array(image) - 127.5 / 127.5)\n",
    "    #\timage = plt.imread(image)\n",
    "    #\tplt.imshow(image)\n",
    "    # plt.show()\n",
    "    datasetlist.append(image)\n",
    "    print(filename)\n",
    "    # print(properties)\n",
    "\n",
    "dataset = np.array(datasetlist)\n",
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "def sampling(args):\n",
    "    mean, logsigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(mean)[0], 512), mean=0., stddev=1.0)\n",
    "    return mean + K.exp(logsigma / 2) * epsilon\n",
    "\n",
    "\n",
    "def encoder(kernel, filter, rows, columns, channel):\n",
    "    X = Input(shape=(rows, columns, channel))\n",
    "    model = Conv2D(filters=filter, kernel_size=kernel, strides=2, padding='same')(X)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Flatten()(model)\n",
    "\n",
    "    mean = Dense(512)(model)\n",
    "    logsigma = Dense(512, activation='tanh')(model)\n",
    "    latent = Lambda(sampling, output_shape=(512,))([mean, logsigma])\n",
    "    meansigma = Model([X], [mean, logsigma, latent])\n",
    "    return meansigma\n",
    "\n",
    "\n",
    "def decgen(kernel, filter, rows, columns, channel):\n",
    "    X = Input(shape=(512,))\n",
    "\n",
    "    model = Dense(filter*8*rows*columns)(X)\n",
    "    model = Reshape((rows, columns, filter * 8))(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(filters=filter, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = Activation('relu')(model)\n",
    "\n",
    "    model = Conv2DTranspose(filters=channel, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = Activation('tanh')(model)\n",
    "\n",
    "    model = Model(X, model)\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator(kernel, filter, rows, columns, channel):\n",
    "    X = Input(shape=(rows, columns, channel))\n",
    "\n",
    "    model = Conv2D(filters=filter*2, kernel_size=kernel, strides=2, padding='same')(X)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*4, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "    model = BatchNormalization(epsilon=1e-5)(model)\n",
    "    model = LeakyReLU(alpha=0.2)(model)\n",
    "\n",
    "    model = Conv2D(filters=filter*8, kernel_size=kernel, strides=2, padding='same')(model)\n",
    "\n",
    "\n",
    "    dec = BatchNormalization(epsilon=1e-5)(model)\n",
    "    dec = LeakyReLU(alpha=0.2)(dec)\n",
    "    dec = Flatten()(dec)\n",
    "    dec = Dense(1, activation='sigmoid')(dec)\n",
    "\n",
    "    output = Model([X], [dec, model])\n",
    "    return output\n",
    "\n",
    "\n",
    "batch_size = 512\n",
    "rows = 64\n",
    "columns = 64\n",
    "channel = 3\n",
    "epochs = 20000\n",
    "datasize = len(dataset)\n",
    "noise = np.random.normal(0, 1, (batch_size, 256))\n",
    "# optimizers\n",
    "SGDop = SGD(lr=0.0003)\n",
    "ADAMop = Adam(lr=0.0002)\n",
    "# encoder\n",
    "E = encoder(5, 32, rows, columns, channel)\n",
    "E.compile(optimizer=SGDop, loss='mse')\n",
    "E.summary()\n",
    "# generator/decoder\n",
    "G = decgen(5, 32, rows, columns, channel)\n",
    "G.compile(optimizer=SGDop, loss='mse')\n",
    "G.summary()\n",
    "# discriminator\n",
    "D = discriminator(5, 32, rows, columns, channel)\n",
    "D.compile(optimizer=SGDop, loss='mse')\n",
    "D.summary()\n",
    "D_fixed = discriminator(5, 32, rows, columns, channel)\n",
    "D_fixed.compile(optimizer=SGDop, loss='mse')\n",
    "# VAE\n",
    "X = Input(shape=(rows, columns, channel))\n",
    "# latent_rep = E(X)[0]\n",
    "# output = G(latent_rep)\n",
    "E_mean, E_logsigma, Z = E(X)\n",
    "\n",
    "# Z = Input(shape=(512,))\n",
    "# Z2 = Input(shape=(batch_size, 512))\n",
    "\n",
    "output = G(Z)\n",
    "G_dec = G(E_mean + E_logsigma)\n",
    "D_fake, F_fake = D(output)\n",
    "D_fromGen, F_fromGen = D(G_dec)\n",
    "D_true, F_true = D(X)\n",
    "\n",
    "VAE = Model(X, output)\n",
    "kl = - 0.5 * K.sum(1 + E_logsigma - K.square(E_mean) - K.exp(E_logsigma), axis=-1)\n",
    "crossent = 64 * metrics.mse(K.flatten(X), K.flatten(output))\n",
    "VAEloss = K.mean(crossent + kl)\n",
    "VAE.add_loss(VAEloss)\n",
    "VAE.compile(optimizer=SGDop)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    latent_vect = E.predict(dataset)[0]\n",
    "    encImg = G.predict(latent_vect)\n",
    "    fakeImg = G.predict(noise)\n",
    "\n",
    "    DlossTrue = D_true.train_on_batch(dataset, np.ones((batch_size, 1)))\n",
    "    DlossEnc = D_fromGen.train_on_batch(encImg, np.ones((batch_size, 1)))\n",
    "    DlossFake = D_fake.train_on_batch(fakeImg, np.zeros((batch_size, 1)))\n",
    "\n",
    "    cnt = epoch\n",
    "    while cnt > 3:\n",
    "        cnt = cnt - 4\n",
    "\n",
    "    if cnt == 0:\n",
    "        GlossEnc = G.train_on_batch(latent_vect, np.ones((batch_size, 1)))\n",
    "        GlossGen = G.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "        Eloss = VAE.train_on_batch(dataset, None)\n",
    "\n",
    "    chk = epoch\n",
    "\n",
    "    while chk > 50:\n",
    "        chk = chk - 51\n",
    "\n",
    "    if chk == 0:\n",
    "        D.save_weights('discriminator.h5')\n",
    "        G.save_weights('generator.h5')\n",
    "        E.save_weights('encoder.h5')\n",
    "\n",
    "    print(\"epoch number\", epoch + 1)\n",
    "    print(\"loss:\")\n",
    "    print(\"D:\", DlossTrue, DlossEnc, DlossFake)\n",
    "    print(\"G:\", GlossEnc, GlossGen)\n",
    "    print(\"VAE:\", Eloss)\n",
    "\n",
    "print('Training done,saving weights')\n",
    "D.save_weights('discriminator.h5')\n",
    "G.save_weights('generator.h5')\n",
    "E.save_weights('encoder.h5')\n",
    "print('end')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
