{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "numSeed = 42\n",
    "np.random.seed(numSeed)\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Lambda, Conv2D, Conv2DTranspose, Activation, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from util import plotScatterDecode, plotProgress, plotCompDecode, plotScatterEncode, addNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 200\n",
    "sizeBatch = 32\n",
    "sizeKernel = 3\n",
    "dimInter1 = 1024\n",
    "dimInter2 = 512\n",
    "dimInter3 = 256\n",
    "dimEncode = 128\n",
    "layer_filters = [8, 32, 128, 512]\n",
    "stdEps = 1.0 \n",
    "ratRecon = 0.998\n",
    "factNoise = 0\n",
    "nameOptim = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(xTrain, _), (xTest, yTest) = cifar10.load_data()\n",
    "yTest = np.squeeze(yTest, axis=1)\n",
    "sizeDigit = xTrain.shape[1]\n",
    "numTrain = len(xTrain)\n",
    "numTest = len(xTest)\n",
    "dimInput = xTrain.shape[1:]\n",
    "\n",
    "xTrain = xTrain.astype('float32') / 255.\n",
    "xTest = xTest.astype('float32') / 255.\n",
    "xTrain\n",
    "\n",
    "xTrain = np.reshape(xTrain, [-1, *dimInput])\n",
    "xTest = np.reshape(xTest, [-1, *dimInput])\n",
    "xTrainNoise = addNoise(xTrain, factNoise=factNoise)\n",
    "xTestNoise = addNoise(xTest, factNoise=factNoise)\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 8)    224         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 32)     2336        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 128)    36992       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 512)    590336      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 2048)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          524800      dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          131328      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          32896       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 128)          0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,449,984\n",
      "Trainable params: 3,449,984\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2048)              2099200   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         589952    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 32)        36896     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 8)         2312      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 3)         219       \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 5,778,307\n",
      "Trainable params: 5,778,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(dimInput))  # adapt this if using `channels_first` image data format\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=sizeKernel,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "x = Dense(dimInter1, activation='relu')(x)\n",
    "x = Dense(dimInter2, activation='relu')(x)\n",
    "x = Dense(dimInter3, activation='relu')(x)\n",
    "zMean = Dense(dimEncode)(x)\n",
    "zSigmaLog = Dense(dimEncode)(x) # log for linear dense\n",
    "\n",
    "def sampling(args):\n",
    "    zMean, zSigmaLog = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(zMean)[0], dimEncode),\n",
    "                              mean=0., stddev=stdEps)\n",
    "    return zMean + K.exp(zSigmaLog) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "z = Lambda(sampling)([zMean, zSigmaLog])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Build the Decoder Model\n",
    "inputLatent = Input(shape=(dimEncode,), name='decoder_input')\n",
    "x = Dense(dimInter3, activation='relu')(inputLatent)\n",
    "x = Dense(dimInter2, activation='relu')(x)\n",
    "x = Dense(dimInter1, activation='relu')(x)\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(x)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=sizeKernel,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=3,\n",
    "                    kernel_size=sizeKernel,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(inputLatent, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 128)               3449984   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 32, 32, 3)         5778307   \n",
      "=================================================================\n",
      "Total params: 9,228,291\n",
      "Trainable params: 9,228,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 22s 434us/step - loss: 0.6612 - val_loss: 0.6507\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 20s 392us/step - loss: 0.6446 - val_loss: 0.6436\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 19s 390us/step - loss: 0.6431 - val_loss: 0.6435\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 20s 403us/step - loss: 0.6424 - val_loss: 0.6417\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 21s 420us/step - loss: 0.6396 - val_loss: 0.6398\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 22s 440us/step - loss: 0.6384 - val_loss: 0.6380\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 22s 447us/step - loss: 0.6370 - val_loss: 0.6372\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 21s 419us/step - loss: 0.6367 - val_loss: 0.6371\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 21s 422us/step - loss: 0.6366 - val_loss: 0.6373\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.6364 - val_loss: 0.6381\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 21s 426us/step - loss: 0.6362 - val_loss: 0.6367\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 21s 429us/step - loss: 0.6360 - val_loss: 0.6363\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 22s 431us/step - loss: 0.6359 - val_loss: 0.6362\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 21s 430us/step - loss: 0.6357 - val_loss: 0.6365\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.6356 - val_loss: 0.6362\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 22s 437us/step - loss: 0.6354 - val_loss: 0.6359\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 0.6353 - val_loss: 0.6361\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 23s 453us/step - loss: 0.6353 - val_loss: 0.6358\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6353 - val_loss: 0.6359\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 22s 435us/step - loss: 0.6352 - val_loss: 0.6361\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 21s 427us/step - loss: 0.6351 - val_loss: 0.6360\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6351 - val_loss: 0.6357\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6349 - val_loss: 0.6373\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6349 - val_loss: 0.6353\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.6348 - val_loss: 0.6356\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.6347 - val_loss: 0.6355\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6345 - val_loss: 0.6351\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6345 - val_loss: 0.6348\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6343 - val_loss: 0.6347\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6342 - val_loss: 0.6345\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6342 - val_loss: 0.6349\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.6340 - val_loss: 0.6350\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.6338 - val_loss: 0.6344\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6338 - val_loss: 0.6344\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6337 - val_loss: 0.6348\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6335 - val_loss: 0.6351\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.6333 - val_loss: 0.6339\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6334 - val_loss: 0.6340\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6330 - val_loss: 0.6341\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6325 - val_loss: 0.6329\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6322 - val_loss: 0.6331\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6321 - val_loss: 0.6327\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6319 - val_loss: 0.6326\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6317 - val_loss: 0.6324\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6317 - val_loss: 0.6327\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.6316 - val_loss: 0.6331\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6315 - val_loss: 0.6324\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6316 - val_loss: 0.6330\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6314 - val_loss: 0.6322\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6313 - val_loss: 0.6324\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.6313 - val_loss: 0.6323\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6314 - val_loss: 0.6328\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 23s 456us/step - loss: 0.6312 - val_loss: 0.6321\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.6312 - val_loss: 0.6321\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 24s 470us/step - loss: 0.6311 - val_loss: 0.6323\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.6311 - val_loss: 0.6319\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6312 - val_loss: 0.6330\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6313 - val_loss: 0.6321\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.6310 - val_loss: 0.6317\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6310 - val_loss: 0.6323\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 23s 469us/step - loss: 0.6309 - val_loss: 0.6322\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6309 - val_loss: 0.6321\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6309 - val_loss: 0.6335\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 28s 551us/step - loss: 0.6308 - val_loss: 0.6320\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 29s 581us/step - loss: 0.6307 - val_loss: 0.6318\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 29s 583us/step - loss: 0.6308 - val_loss: 0.6321\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 29s 585us/step - loss: 0.6308 - val_loss: 0.6324\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 29s 581us/step - loss: 0.6307 - val_loss: 0.6323\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 29s 583us/step - loss: 0.6308 - val_loss: 0.6322\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 29s 580us/step - loss: 0.6307 - val_loss: 0.6324\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 29s 581us/step - loss: 0.6306 - val_loss: 0.6319\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 29s 583us/step - loss: 0.6307 - val_loss: 0.6321\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 29s 585us/step - loss: 0.6306 - val_loss: 0.6329\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 29s 582us/step - loss: 0.6306 - val_loss: 0.6321\n",
      "Epoch 75/200\n",
      "50000/50000 [==============================] - 29s 583us/step - loss: 0.6306 - val_loss: 0.6320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 29s 581us/step - loss: 0.6306 - val_loss: 0.6318\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 30s 600us/step - loss: 0.6306 - val_loss: 0.6315\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 29s 590us/step - loss: 0.6305 - val_loss: 0.6318\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 29s 580us/step - loss: 0.6305 - val_loss: 0.6321\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 27s 543us/step - loss: 0.6304 - val_loss: 0.6326\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 27s 544us/step - loss: 0.6306 - val_loss: 0.6315\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 28s 551us/step - loss: 0.6304 - val_loss: 0.6317\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 28s 565us/step - loss: 0.6304 - val_loss: 0.6318\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 28s 557us/step - loss: 0.6304 - val_loss: 0.6317\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 28s 556us/step - loss: 0.6304 - val_loss: 0.6320\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 27s 539us/step - loss: 0.6304 - val_loss: 0.6316\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 26s 511us/step - loss: 0.6304 - val_loss: 0.6316\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6304 - val_loss: 0.6319\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6304 - val_loss: 0.6314\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6303 - val_loss: 0.6320\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6302 - val_loss: 0.6316\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6303 - val_loss: 0.6326\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6302 - val_loss: 0.6321\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6303 - val_loss: 0.6318\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6302 - val_loss: 0.6323\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6302 - val_loss: 0.6315\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6302 - val_loss: 0.6313\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6301 - val_loss: 0.6321\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6302 - val_loss: 0.6317\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6302 - val_loss: 0.6316\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6304 - val_loss: 0.6313\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6302 - val_loss: 0.6314\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6301 - val_loss: 0.6316\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6301 - val_loss: 0.6321\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6300 - val_loss: 0.6320\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6301 - val_loss: 0.6321\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6301 - val_loss: 0.6318\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6300 - val_loss: 0.6313\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6302 - val_loss: 0.6315\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6300 - val_loss: 0.6315\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6301 - val_loss: 0.6317\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6300 - val_loss: 0.6314\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6299 - val_loss: 0.6314\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6300 - val_loss: 0.6314\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6300 - val_loss: 0.6314\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6300 - val_loss: 0.6315\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6299 - val_loss: 0.6318\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6300 - val_loss: 0.6318\n",
      "Epoch 119/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6299 - val_loss: 0.6323\n",
      "Epoch 120/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6300 - val_loss: 0.6317\n",
      "Epoch 121/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6299 - val_loss: 0.6318\n",
      "Epoch 122/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6299 - val_loss: 0.6314\n",
      "Epoch 123/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6299 - val_loss: 0.6314\n",
      "Epoch 124/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6299 - val_loss: 0.6318\n",
      "Epoch 125/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6300 - val_loss: 0.6316\n",
      "Epoch 126/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6314\n",
      "Epoch 127/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6299 - val_loss: 0.6315\n",
      "Epoch 128/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6300 - val_loss: 0.6314\n",
      "Epoch 129/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6298 - val_loss: 0.6319\n",
      "Epoch 130/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6300 - val_loss: 0.6328\n",
      "Epoch 131/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6315\n",
      "Epoch 132/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6298 - val_loss: 0.6313\n",
      "Epoch 133/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6298 - val_loss: 0.6327\n",
      "Epoch 134/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6297 - val_loss: 0.6314\n",
      "Epoch 135/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6312\n",
      "Epoch 136/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6298 - val_loss: 0.6315\n",
      "Epoch 137/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6299 - val_loss: 0.6313\n",
      "Epoch 138/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6298 - val_loss: 0.6316\n",
      "Epoch 139/200\n",
      "50000/50000 [==============================] - 23s 456us/step - loss: 0.6297 - val_loss: 0.6312\n",
      "Epoch 140/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6313\n",
      "Epoch 141/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6298 - val_loss: 0.6316\n",
      "Epoch 142/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6297 - val_loss: 0.6317\n",
      "Epoch 143/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6297 - val_loss: 0.6315\n",
      "Epoch 144/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6322\n",
      "Epoch 145/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6297 - val_loss: 0.6318\n",
      "Epoch 146/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6297 - val_loss: 0.6316\n",
      "Epoch 147/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6298 - val_loss: 0.6313\n",
      "Epoch 148/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6298 - val_loss: 0.6316\n",
      "Epoch 149/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6298 - val_loss: 0.6318\n",
      "Epoch 150/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6297 - val_loss: 0.6315\n",
      "Epoch 151/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6297 - val_loss: 0.6313\n",
      "Epoch 152/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6297 - val_loss: 0.6317\n",
      "Epoch 153/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6296 - val_loss: 0.6319\n",
      "Epoch 154/200\n",
      "50000/50000 [==============================] - 23s 457us/step - loss: 0.6300 - val_loss: 0.6313\n",
      "Epoch 155/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6296 - val_loss: 0.6313\n",
      "Epoch 156/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6296 - val_loss: 0.6329\n",
      "Epoch 157/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6297 - val_loss: 0.6313\n",
      "Epoch 158/200\n",
      "50000/50000 [==============================] - 23s 467us/step - loss: 0.6295 - val_loss: 0.6319\n",
      "Epoch 159/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6296 - val_loss: 0.6312\n",
      "Epoch 160/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6299 - val_loss: 0.6313\n",
      "Epoch 161/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6296 - val_loss: 0.6323\n",
      "Epoch 162/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6297 - val_loss: 0.6314\n",
      "Epoch 163/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 164/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6297 - val_loss: 0.6311\n",
      "Epoch 165/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6296 - val_loss: 0.6319\n",
      "Epoch 166/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6296 - val_loss: 0.6313\n",
      "Epoch 167/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6295 - val_loss: 0.6314\n",
      "Epoch 168/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6295 - val_loss: 0.6314\n",
      "Epoch 169/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6296 - val_loss: 0.6311\n",
      "Epoch 170/200\n",
      "50000/50000 [==============================] - 23s 459us/step - loss: 0.6296 - val_loss: 0.6314\n",
      "Epoch 171/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6296 - val_loss: 0.6314\n",
      "Epoch 172/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6296 - val_loss: 0.6313\n",
      "Epoch 173/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6296 - val_loss: 0.6315\n",
      "Epoch 174/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6295 - val_loss: 0.6316\n",
      "Epoch 175/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6296 - val_loss: 0.6315\n",
      "Epoch 176/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6295 - val_loss: 0.6312\n",
      "Epoch 177/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 178/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6296 - val_loss: 0.6313\n",
      "Epoch 179/200\n",
      "50000/50000 [==============================] - 23s 458us/step - loss: 0.6296 - val_loss: 0.6314\n",
      "Epoch 180/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6294 - val_loss: 0.6316\n",
      "Epoch 181/200\n",
      "50000/50000 [==============================] - 23s 455us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 182/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6297 - val_loss: 0.6321\n",
      "Epoch 183/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 184/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6295 - val_loss: 0.6316\n",
      "Epoch 185/200\n",
      "50000/50000 [==============================] - 23s 460us/step - loss: 0.6294 - val_loss: 0.6314\n",
      "Epoch 186/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6295 - val_loss: 0.6315\n",
      "Epoch 187/200\n",
      "50000/50000 [==============================] - 23s 461us/step - loss: 0.6294 - val_loss: 0.6313\n",
      "Epoch 188/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6294 - val_loss: 0.6314\n",
      "Epoch 189/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6294 - val_loss: 0.6317\n",
      "Epoch 190/200\n",
      "50000/50000 [==============================] - 23s 462us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 191/200\n",
      "50000/50000 [==============================] - 23s 468us/step - loss: 0.6295 - val_loss: 0.6313\n",
      "Epoch 192/200\n",
      "50000/50000 [==============================] - 23s 465us/step - loss: 0.6296 - val_loss: 0.6313\n",
      "Epoch 193/200\n",
      "50000/50000 [==============================] - 23s 463us/step - loss: 0.6295 - val_loss: 0.6315\n",
      "Epoch 194/200\n",
      " 6112/50000 [==>...........................] - ETA: 19s - loss: 0.6285"
     ]
    }
   ],
   "source": [
    "def lossVAE(zMean, zSigmaLog):\n",
    "    def loss(tensorInput, tensorDecode):\n",
    "        lossRecon =  metrics.binary_crossentropy(K.flatten(tensorInput), K.flatten(tensorDecode))\n",
    "        lossKL = - 0.5 * K.sum(1 + 2 * zSigmaLog - K.square(zMean) - K.square(K.exp(zSigmaLog)), axis=-1)\n",
    "#         lossKL = - 0.5 * K.mean(1 + zSigmaLog - K.square(zMean) - K.exp(zSigmaLog), axis=-1)\n",
    "        return ratRecon * lossRecon + (1 - ratRecon) * lossKL\n",
    "    return loss\n",
    "\n",
    "autoencoder.compile(optimizer=nameOptim, loss=lossVAE(zMean, zSigmaLog))\n",
    "\n",
    "# Train the autoencoder\n",
    "tic = time()\n",
    "history = autoencoder.fit(xTrainNoise, xTrain,\n",
    "                epochs=numEpochs,\n",
    "                batch_size=sizeBatch,\n",
    "                shuffle=True,\n",
    "                validation_data=(xTest, xTest))\n",
    "timeTrain = time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the historical training progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"traing consumed: \" + str(timeTrain) + \" seconds\")\n",
    "plotProgress(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the encoding and decoding results of testing data, and get the mean/std of the encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encodeTest = encoder.predict(xTestNoise)\n",
    "decodeTest = decoder.predict(encodeTest)\n",
    "meanEncTest = np.mean(encodeTest, axis=0)\n",
    "stdEncTest = np.std(encodeTest, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original digitals with the decoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCompDecode(xTest, decodeTest, sizeDigit = (32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the scatter of the encoding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlim = (meanEncTest[0] - 4*stdEncTest[0], meanEncTest[0] + 4*stdEncTest[0])\n",
    "ylim = (meanEncTest[1] - 4*stdEncTest[1], meanEncTest[1] + 4*stdEncTest[1])\n",
    "\n",
    "plotScatterEncode(encodeTest, yTest, xlim, ylim, numShow=10000)\n",
    "scoreSilh = silhouette_score(encodeTest, yTest)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decoding results from the encoding scatter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "plotScatterDecode(decoder, (32,32,3), xlim, ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeTrain, history.history[\"loss\"][numEpochs-1], history.history[\"val_loss\"][numEpochs-1], scoreSilh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, _), (xTest, yTest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*dimInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
