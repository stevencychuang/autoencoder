{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "numSeed = 42\n",
    "np.random.seed(numSeed)\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Dense, Lambda, Conv2D, Conv2DTranspose, Activation, Flatten, Reshape\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.48\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from util import plotScatterDecode, plotProgress, plotCompDecode, plotScatterEncode, addNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numEpochs = 200\n",
    "sizeBatch = 32\n",
    "sizeKernel = 3\n",
    "dimInter1 = 256\n",
    "dimInter2 = 64\n",
    "dimInter3 = 16\n",
    "dimEncode = 4\n",
    "layer_filters = [8, 32, 128, 256]\n",
    "stdEps = 1.0 \n",
    "ratRecon = 0.998\n",
    "factNoise = 0\n",
    "nameOptim = 'adam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "(xTrain, _), (xTest, yTest) = cifar10.load_data()\n",
    "yTest = np.squeeze(yTest, axis=1)\n",
    "sizeDigit = xTrain.shape[1]\n",
    "numTrain = len(xTrain)\n",
    "numTest = len(xTest)\n",
    "dimInput = xTrain.shape[1:]\n",
    "\n",
    "xTrain = xTrain.astype('float32') / 255.\n",
    "xTest = xTest.astype('float32') / 255.\n",
    "xTrain\n",
    "\n",
    "xTrain = np.reshape(xTrain, [-1, *dimInput])\n",
    "xTest = np.reshape(xTest, [-1, *dimInput])\n",
    "xTrainNoise = addNoise(xTrain, factNoise=factNoise)\n",
    "xTestNoise = addNoise(xTest, factNoise=factNoise)\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 16, 16, 8)    224         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 8, 32)     2336        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 4, 4, 128)    36992       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 256)    295168      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1024)         0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          262400      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 64)           16448       dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           1040        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            68          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 4)            68          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 614,744\n",
      "Trainable params: 614,744\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "decoder_input (InputLayer)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                1088      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              263168    \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 8, 8, 128)         295040    \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTr (None, 16, 16, 32)        36896     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTr (None, 32, 32, 8)         2312      \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 32, 32, 3)         219       \n",
      "_________________________________________________________________\n",
      "decoder_output (Activation)  (None, 32, 32, 3)         0         \n",
      "=================================================================\n",
      "Total params: 1,205,523\n",
      "Trainable params: 1,205,523\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(dimInput))  # adapt this if using `channels_first` image data format\n",
    "x = inputs\n",
    "# Stack of Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use MaxPooling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters:\n",
    "    x = Conv2D(filters=filters,\n",
    "               kernel_size=sizeKernel,\n",
    "               strides=2,\n",
    "               activation='relu',\n",
    "               padding='same')(x)\n",
    "\n",
    "# Shape info needed to build Decoder Model\n",
    "shape = K.int_shape(x)\n",
    "\n",
    "# Generate the latent vector\n",
    "x = Flatten()(x)\n",
    "x = Dense(dimInter1, activation='relu')(x)\n",
    "x = Dense(dimInter2, activation='relu')(x)\n",
    "x = Dense(dimInter3, activation='relu')(x)\n",
    "zMean = Dense(dimEncode)(x)\n",
    "zSigmaLog = Dense(dimEncode)(x) # log for linear dense\n",
    "\n",
    "def sampling(args):\n",
    "    zMean, zSigmaLog = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(zMean)[0], dimEncode),\n",
    "                              mean=0., stddev=stdEps)\n",
    "    return zMean + K.exp(zSigmaLog) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "# z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "z = Lambda(sampling)([zMean, zSigmaLog])\n",
    "encoder = Model(inputs, z, name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# Build the Decoder Model\n",
    "inputLatent = Input(shape=(dimEncode,), name='decoder_input')\n",
    "x = Dense(dimInter3, activation='relu')(inputLatent)\n",
    "x = Dense(dimInter2, activation='relu')(x)\n",
    "x = Dense(dimInter1, activation='relu')(x)\n",
    "x = Dense(shape[1] * shape[2] * shape[3])(x)\n",
    "x = Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "\n",
    "# Stack of Transposed Conv2D blocks\n",
    "# Notes:\n",
    "# 1) Use Batch Normalization before ReLU on deep networks\n",
    "# 2) Use UpSampling2D as alternative to strides>1\n",
    "# - faster but not as good as strides>1\n",
    "for filters in layer_filters[::-1]:\n",
    "    x = Conv2DTranspose(filters=filters,\n",
    "                        kernel_size=sizeKernel,\n",
    "                        strides=2,\n",
    "                        activation='relu',\n",
    "                        padding='same')(x)\n",
    "\n",
    "x = Conv2DTranspose(filters=3,\n",
    "                    kernel_size=sizeKernel,\n",
    "                    padding='same')(x)\n",
    "\n",
    "outputs = Activation('sigmoid', name='decoder_output')(x)\n",
    "\n",
    "# Instantiate Decoder Model\n",
    "decoder = Model(inputLatent, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              (None, 4)                 614744    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 32, 32, 3)         1205523   \n",
      "=================================================================\n",
      "Total params: 1,820,267\n",
      "Trainable params: 1,820,267\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder = Encoder + Decoder\n",
    "# Instantiate Autoencoder Model\n",
    "autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "50000/50000 [==============================] - 29s 574us/step - loss: 0.6571 - val_loss: 0.6442\n",
      "Epoch 2/200\n",
      "50000/50000 [==============================] - 25s 504us/step - loss: 0.6383 - val_loss: 0.6375\n",
      "Epoch 3/200\n",
      "50000/50000 [==============================] - 25s 508us/step - loss: 0.6369 - val_loss: 0.6372\n",
      "Epoch 4/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6364 - val_loss: 0.6371\n",
      "Epoch 5/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6362 - val_loss: 0.6366\n",
      "Epoch 6/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6359 - val_loss: 0.6359\n",
      "Epoch 7/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6357 - val_loss: 0.6361\n",
      "Epoch 8/200\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.6355 - val_loss: 0.6361\n",
      "Epoch 9/200\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.6354 - val_loss: 0.6364\n",
      "Epoch 10/200\n",
      "50000/50000 [==============================] - 26s 517us/step - loss: 0.6353 - val_loss: 0.6357\n",
      "Epoch 11/200\n",
      "50000/50000 [==============================] - 26s 518us/step - loss: 0.6351 - val_loss: 0.6357\n",
      "Epoch 12/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6351 - val_loss: 0.6357\n",
      "Epoch 13/200\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6350 - val_loss: 0.6359\n",
      "Epoch 14/200\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6349 - val_loss: 0.6356\n",
      "Epoch 15/200\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6348 - val_loss: 0.6353\n",
      "Epoch 16/200\n",
      "50000/50000 [==============================] - 24s 487us/step - loss: 0.6347 - val_loss: 0.6356\n",
      "Epoch 17/200\n",
      "50000/50000 [==============================] - 24s 487us/step - loss: 0.6346 - val_loss: 0.6355\n",
      "Epoch 18/200\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6344 - val_loss: 0.6349\n",
      "Epoch 19/200\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.6343 - val_loss: 0.6351\n",
      "Epoch 20/200\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.6342 - val_loss: 0.6351\n",
      "Epoch 21/200\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6341 - val_loss: 0.6355\n",
      "Epoch 22/200\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6342 - val_loss: 0.6356\n",
      "Epoch 23/200\n",
      "50000/50000 [==============================] - 25s 503us/step - loss: 0.6339 - val_loss: 0.6347\n",
      "Epoch 24/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6341 - val_loss: 0.6346\n",
      "Epoch 25/200\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.6340 - val_loss: 0.6348\n",
      "Epoch 26/200\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.6340 - val_loss: 0.6352\n",
      "Epoch 27/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6341 - val_loss: 0.6352\n",
      "Epoch 28/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6339 - val_loss: 0.6347\n",
      "Epoch 29/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6339 - val_loss: 0.6347\n",
      "Epoch 30/200\n",
      "50000/50000 [==============================] - 26s 510us/step - loss: 0.6339 - val_loss: 0.6344\n",
      "Epoch 31/200\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.6339 - val_loss: 0.6359\n",
      "Epoch 32/200\n",
      "50000/50000 [==============================] - 26s 512us/step - loss: 0.6340 - val_loss: 0.6345\n",
      "Epoch 33/200\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 0.6340 - val_loss: 0.6355\n",
      "Epoch 34/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6339 - val_loss: 0.6346\n",
      "Epoch 35/200\n",
      "50000/50000 [==============================] - 25s 497us/step - loss: 0.6340 - val_loss: 0.6344\n",
      "Epoch 36/200\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6338 - val_loss: 0.6346\n",
      "Epoch 37/200\n",
      "50000/50000 [==============================] - 23s 464us/step - loss: 0.6338 - val_loss: 0.6346\n",
      "Epoch 38/200\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6346 - val_loss: 0.6347\n",
      "Epoch 39/200\n",
      "50000/50000 [==============================] - 24s 486us/step - loss: 0.6337 - val_loss: 0.6347\n",
      "Epoch 40/200\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6338 - val_loss: 0.6347\n",
      "Epoch 41/200\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6336 - val_loss: 0.6349\n",
      "Epoch 42/200\n",
      "50000/50000 [==============================] - 25s 505us/step - loss: 0.6339 - val_loss: 0.6345\n",
      "Epoch 43/200\n",
      "50000/50000 [==============================] - 25s 490us/step - loss: 0.6336 - val_loss: 0.6344\n",
      "Epoch 44/200\n",
      "50000/50000 [==============================] - 24s 484us/step - loss: 0.6337 - val_loss: 0.6352\n",
      "Epoch 45/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6338 - val_loss: 0.6457\n",
      "Epoch 46/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6343 - val_loss: 0.6359\n",
      "Epoch 47/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6344 - val_loss: 0.6351\n",
      "Epoch 48/200\n",
      "50000/50000 [==============================] - 26s 526us/step - loss: 0.6341 - val_loss: 0.6352\n",
      "Epoch 49/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6336 - val_loss: 0.6356\n",
      "Epoch 50/200\n",
      "50000/50000 [==============================] - 26s 529us/step - loss: 0.6335 - val_loss: 0.6342\n",
      "Epoch 51/200\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.6344 - val_loss: 0.6357\n",
      "Epoch 52/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6338 - val_loss: 0.6346\n",
      "Epoch 53/200\n",
      "50000/50000 [==============================] - 27s 536us/step - loss: 0.6336 - val_loss: 0.6344\n",
      "Epoch 54/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6336 - val_loss: 0.6345\n",
      "Epoch 55/200\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.6335 - val_loss: 0.6342\n",
      "Epoch 56/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6334 - val_loss: 0.6342\n",
      "Epoch 57/200\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6337 - val_loss: 0.6345\n",
      "Epoch 58/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6340 - val_loss: 0.6350\n",
      "Epoch 59/200\n",
      "50000/50000 [==============================] - 24s 483us/step - loss: 0.6337 - val_loss: 0.6342\n",
      "Epoch 60/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6336 - val_loss: 0.6344\n",
      "Epoch 61/200\n",
      "50000/50000 [==============================] - 23s 466us/step - loss: 0.6338 - val_loss: 0.6344\n",
      "Epoch 62/200\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6337 - val_loss: 0.6341\n",
      "Epoch 63/200\n",
      "50000/50000 [==============================] - 24s 471us/step - loss: 0.6335 - val_loss: 0.6344\n",
      "Epoch 64/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6334 - val_loss: 0.6342\n",
      "Epoch 65/200\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6337 - val_loss: 0.6341\n",
      "Epoch 66/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6336 - val_loss: 0.6343\n",
      "Epoch 67/200\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6342 - val_loss: 0.6343\n",
      "Epoch 68/200\n",
      "50000/50000 [==============================] - 24s 474us/step - loss: 0.6337 - val_loss: 0.6354\n",
      "Epoch 69/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6344 - val_loss: 0.6344\n",
      "Epoch 70/200\n",
      "50000/50000 [==============================] - 24s 472us/step - loss: 0.6339 - val_loss: 0.6349\n",
      "Epoch 71/200\n",
      "50000/50000 [==============================] - 25s 493us/step - loss: 0.6340 - val_loss: 0.6352\n",
      "Epoch 72/200\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.6336 - val_loss: 0.6340\n",
      "Epoch 73/200\n",
      "50000/50000 [==============================] - 26s 527us/step - loss: 0.6338 - val_loss: 0.6343\n",
      "Epoch 74/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6349 - val_loss: 0.6390\n",
      "Epoch 75/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 25s 498us/step - loss: 0.6348 - val_loss: 0.6352\n",
      "Epoch 76/200\n",
      "50000/50000 [==============================] - 24s 481us/step - loss: 0.6339 - val_loss: 0.6351\n",
      "Epoch 77/200\n",
      "50000/50000 [==============================] - 26s 511us/step - loss: 0.6342 - val_loss: 0.6345\n",
      "Epoch 78/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6338 - val_loss: 0.6345\n",
      "Epoch 79/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6338 - val_loss: 0.6346\n",
      "Epoch 80/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6337 - val_loss: 0.6348\n",
      "Epoch 81/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6340 - val_loss: 0.6345\n",
      "Epoch 82/200\n",
      "50000/50000 [==============================] - 24s 478us/step - loss: 0.6337 - val_loss: 0.6341\n",
      "Epoch 83/200\n",
      "50000/50000 [==============================] - 24s 479us/step - loss: 0.6338 - val_loss: 0.6342\n",
      "Epoch 84/200\n",
      "50000/50000 [==============================] - 25s 507us/step - loss: 0.6335 - val_loss: 0.6350\n",
      "Epoch 85/200\n",
      "50000/50000 [==============================] - 25s 507us/step - loss: 0.6338 - val_loss: 0.6345\n",
      "Epoch 86/200\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.6334 - val_loss: 0.6348\n",
      "Epoch 87/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6337 - val_loss: 0.6345\n",
      "Epoch 88/200\n",
      "50000/50000 [==============================] - 24s 480us/step - loss: 0.6334 - val_loss: 0.6348\n",
      "Epoch 89/200\n",
      "50000/50000 [==============================] - 24s 487us/step - loss: 0.6337 - val_loss: 0.6346\n",
      "Epoch 90/200\n",
      "50000/50000 [==============================] - 24s 490us/step - loss: 0.6342 - val_loss: 0.6353\n",
      "Epoch 91/200\n",
      "50000/50000 [==============================] - 26s 511us/step - loss: 0.6341 - val_loss: 0.6347\n",
      "Epoch 92/200\n",
      "50000/50000 [==============================] - 25s 491us/step - loss: 0.6344 - val_loss: 0.6348\n",
      "Epoch 93/200\n",
      "50000/50000 [==============================] - 25s 496us/step - loss: 0.6336 - val_loss: 0.6346\n",
      "Epoch 94/200\n",
      "50000/50000 [==============================] - 26s 510us/step - loss: 0.6339 - val_loss: 0.6343\n",
      "Epoch 95/200\n",
      "50000/50000 [==============================] - 26s 515us/step - loss: 0.6335 - val_loss: 0.6352\n",
      "Epoch 96/200\n",
      "50000/50000 [==============================] - 26s 521us/step - loss: 0.6333 - val_loss: 0.6342\n",
      "Epoch 97/200\n",
      "50000/50000 [==============================] - 26s 519us/step - loss: 0.6340 - val_loss: 0.6361\n",
      "Epoch 98/200\n",
      "50000/50000 [==============================] - 26s 522us/step - loss: 0.6347 - val_loss: 0.6349\n",
      "Epoch 99/200\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.6337 - val_loss: 0.6344\n",
      "Epoch 100/200\n",
      "50000/50000 [==============================] - 26s 520us/step - loss: 0.6338 - val_loss: 0.6352\n",
      "Epoch 101/200\n",
      "50000/50000 [==============================] - 25s 508us/step - loss: 0.6337 - val_loss: 0.6351\n",
      "Epoch 102/200\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.6338 - val_loss: 0.6345\n",
      "Epoch 103/200\n",
      "50000/50000 [==============================] - 26s 512us/step - loss: 0.6334 - val_loss: 0.6339\n",
      "Epoch 104/200\n",
      "50000/50000 [==============================] - 26s 516us/step - loss: 0.6338 - val_loss: 0.6351\n",
      "Epoch 105/200\n",
      "50000/50000 [==============================] - 26s 511us/step - loss: 0.6337 - val_loss: 0.6344\n",
      "Epoch 106/200\n",
      "50000/50000 [==============================] - 26s 517us/step - loss: 0.6337 - val_loss: 0.6343\n",
      "Epoch 107/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6337 - val_loss: 0.6347\n",
      "Epoch 108/200\n",
      "50000/50000 [==============================] - 26s 514us/step - loss: 0.6337 - val_loss: 0.6340\n",
      "Epoch 109/200\n",
      "50000/50000 [==============================] - 25s 505us/step - loss: 0.6335 - val_loss: 0.6347\n",
      "Epoch 110/200\n",
      "50000/50000 [==============================] - 26s 511us/step - loss: 0.6333 - val_loss: 0.6339\n",
      "Epoch 111/200\n",
      "50000/50000 [==============================] - 26s 523us/step - loss: 0.6336 - val_loss: 0.6347\n",
      "Epoch 112/200\n",
      "50000/50000 [==============================] - 26s 513us/step - loss: 0.6344 - val_loss: 0.6372\n",
      "Epoch 113/200\n",
      "50000/50000 [==============================] - 26s 512us/step - loss: 0.6344 - val_loss: 0.6394\n",
      "Epoch 114/200\n",
      "50000/50000 [==============================] - 25s 507us/step - loss: 0.6342 - val_loss: 0.6341\n",
      "Epoch 115/200\n",
      "50000/50000 [==============================] - 24s 476us/step - loss: 0.6336 - val_loss: 0.6338\n",
      "Epoch 116/200\n",
      "50000/50000 [==============================] - 23s 470us/step - loss: 0.6336 - val_loss: 0.6345\n",
      "Epoch 117/200\n",
      "50000/50000 [==============================] - 24s 473us/step - loss: 0.6336 - val_loss: 0.6350\n",
      "Epoch 118/200\n",
      "50000/50000 [==============================] - 24s 489us/step - loss: 0.6336 - val_loss: 0.6351\n",
      "Epoch 119/200\n",
      " 8608/50000 [====>.........................] - ETA: 19s - loss: 0.6335"
     ]
    }
   ],
   "source": [
    "def lossVAE(zMean, zSigmaLog):\n",
    "    def loss(tensorInput, tensorDecode):\n",
    "        lossRecon =  metrics.binary_crossentropy(K.flatten(tensorInput), K.flatten(tensorDecode))\n",
    "        lossKL = - 0.5 * K.sum(1 + 2 * zSigmaLog - K.square(zMean) - K.square(K.exp(zSigmaLog)), axis=-1)\n",
    "#         lossKL = - 0.5 * K.mean(1 + zSigmaLog - K.square(zMean) - K.exp(zSigmaLog), axis=-1)\n",
    "        return ratRecon * lossRecon + (1 - ratRecon) * lossKL\n",
    "    return loss\n",
    "\n",
    "autoencoder.compile(optimizer=nameOptim, loss=lossVAE(zMean, zSigmaLog))\n",
    "\n",
    "# Train the autoencoder\n",
    "tic = time()\n",
    "history = autoencoder.fit(xTrainNoise, xTrain,\n",
    "                epochs=numEpochs,\n",
    "                batch_size=sizeBatch,\n",
    "                shuffle=True,\n",
    "                validation_data=(xTest, xTest))\n",
    "timeTrain = time() - tic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the historical training progress "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"traing consumed: \" + str(timeTrain) + \" seconds\")\n",
    "plotProgress(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the encoding and decoding results of testing data, and get the mean/std of the encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encodeTest = encoder.predict(xTestNoise)\n",
    "decodeTest = decoder.predict(encodeTest)\n",
    "meanEncTest = np.mean(encodeTest, axis=0)\n",
    "stdEncTest = np.std(encodeTest, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original digitals with the decoding results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCompDecode(xTest, decodeTest, sizeDigit = (32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the scatter of the encoding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xlim = (meanEncTest[0] - 4*stdEncTest[0], meanEncTest[0] + 4*stdEncTest[0])\n",
    "ylim = (meanEncTest[1] - 4*stdEncTest[1], meanEncTest[1] + 4*stdEncTest[1])\n",
    "\n",
    "plotScatterEncode(encodeTest, yTest, xlim, ylim, numShow=10000)\n",
    "scoreSilh = silhouette_score(encodeTest, yTest)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decoding results from the encoding scatter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D manifold of the digits\n",
    "plotScatterDecode(decoder, (32,32,3), xlim, ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timeTrain, history.history[\"loss\"][numEpochs-1], history.history[\"val_loss\"][numEpochs-1], scoreSilh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain, _), (xTest, yTest) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
